{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# RESUMO DAS MELHORIAS IMPLEMENTADAS\n",
    "\n",
    "## Experimento 1 (Baseline)\n",
    "- **Arquitetura**: U-Net com encoder ResNet50 pré-treinado (ImageNet)\n",
    "- **Data Augmentation**: Básico (flip, rotação, brilho/contraste)\n",
    "- **Loss**: BCE + Dice\n",
    "- **Scheduler**: CosineAnnealing\n",
    "\n",
    "## Experimento 2 (Melhoria 1)\n",
    "**Foco: Pré-processamento, Data Augmentation e Regularização**\n",
    "\n",
    "| Melhoria | Descrição |\n",
    "|----------|-----------|\n",
    "| **CLAHE** | Contrast Limited Adaptive Histogram Equalization no canal de luminância para realçar estruturas em imagens de fundo de olho |\n",
    "| **Data Aug Médico** | ElasticTransform, GridDistortion, OpticalDistortion para simular variações anatômicas |\n",
    "| **CoarseDropout** | Simula oclusões parciais para regularização |\n",
    "| **Deep Supervision** | Múltiplas saídas em diferentes escalas para melhor fluxo de gradiente |\n",
    "| **TTA** | Test Time Augmentation com 7 transformações para predição mais robusta |\n",
    "| **Scheduler** | CosineAnnealingWarmRestarts para melhor convergência |\n",
    "\n",
    "## Experimento 3 (Melhoria 2 - Arquitetura)\n",
    "**Foco: Modificação na Topologia da Rede Neural**\n",
    "\n",
    "| Modificação | Descrição |\n",
    "|-------------|-----------|\n",
    "| **Attention Gates** | Mecanismos de atenção nas skip connections que permitem ao modelo focar nas regiões relevantes, suprimindo respostas irrelevantes |\n",
    "| **SE Blocks** | Squeeze-and-Excitation blocks para recalibração adaptativa dos canais de features |\n",
    "| **ASPP** | Atrous Spatial Pyramid Pooling no bottleneck para captura de contexto multi-escala |\n",
    "| **Focal Loss** | Adicionada à função de loss para lidar com desbalanceamento de classes |\n",
    "| **Discriminative LR** | Learning rate diferenciado para encoder (menor) e decoder (maior) |\n",
    "| **OneCycleLR** | Scheduler mais agressivo para melhor generalização |\n",
    "\n",
    "---\n",
    "**Nota**: O Experimento 3 implementa uma modificação substancial na arquitetura da rede, não apenas um aumento de largura ou profundidade, atendendo ao requisito de otimização na topologia da rede neural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização comparativa das predições\n",
    "def compare_predictions(dataset, idx=0):\n",
    "    \"\"\"Compara predições dos 3 modelos lado a lado\"\"\"\n",
    "    img, mask = dataset[idx]\n",
    "    img_tensor = img.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Carregar modelos\n",
    "    model.load_state_dict(torch.load('best_optic_disc_model.pth'))\n",
    "    model.eval()\n",
    "    exp2_model.load_state_dict(torch.load('best_exp2_model.pth'))\n",
    "    exp2_model.eval()\n",
    "    exp3_model.load_state_dict(torch.load('best_exp3_attention_unet.pth'))\n",
    "    exp3_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred1 = torch.sigmoid(model(img_tensor)).cpu().squeeze().numpy()\n",
    "        pred2 = torch.sigmoid(exp2_model(img_tensor)).cpu().squeeze().numpy()\n",
    "        pred3 = torch.sigmoid(exp3_model(img_tensor)).cpu().squeeze().numpy()\n",
    "    \n",
    "    # Desnormalizar imagem\n",
    "    img_np = img.numpy().transpose(1, 2, 0)\n",
    "    img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "    \n",
    "    mask_np = mask.squeeze().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    \n",
    "    # Linha 1: Imagem, GT e predições\n",
    "    axes[0, 0].imshow(img_np)\n",
    "    axes[0, 0].set_title('Imagem Original')\n",
    "    \n",
    "    axes[0, 1].imshow(mask_np, cmap='gray')\n",
    "    axes[0, 1].set_title('Ground Truth')\n",
    "    \n",
    "    axes[0, 2].imshow(pred1 > 0.5, cmap='gray')\n",
    "    axes[0, 2].set_title('Exp1: Baseline')\n",
    "    \n",
    "    axes[0, 3].imshow(pred2 > 0.5, cmap='gray')\n",
    "    axes[0, 3].set_title('Exp2: CLAHE+DS')\n",
    "    \n",
    "    # Linha 2: Overlays\n",
    "    overlay_gt = img_np.copy()\n",
    "    overlay_gt[mask_np > 0.5] = overlay_gt[mask_np > 0.5] * 0.5 + np.array([0, 1, 0]) * 0.5\n",
    "    axes[1, 0].imshow(overlay_gt)\n",
    "    axes[1, 0].set_title('Overlay GT')\n",
    "    \n",
    "    overlay1 = img_np.copy()\n",
    "    overlay1[pred1 > 0.5] = overlay1[pred1 > 0.5] * 0.5 + np.array([0, 0, 1]) * 0.5\n",
    "    axes[1, 1].imshow(overlay1)\n",
    "    axes[1, 1].set_title('Overlay Exp1')\n",
    "    \n",
    "    overlay2 = img_np.copy()\n",
    "    overlay2[pred2 > 0.5] = overlay2[pred2 > 0.5] * 0.5 + np.array([1, 0.5, 0]) * 0.5\n",
    "    axes[1, 2].imshow(overlay2)\n",
    "    axes[1, 2].set_title('Overlay Exp2')\n",
    "    \n",
    "    overlay3 = img_np.copy()\n",
    "    overlay3[pred3 > 0.5] = overlay3[pred3 > 0.5] * 0.5 + np.array([1, 0, 1]) * 0.5\n",
    "    axes[1, 3].imshow(overlay3)\n",
    "    axes[1, 3].set_title('Overlay Exp3: Attention')\n",
    "    \n",
    "    for ax in axes.flat:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Comparação das Predições dos 3 Experimentos', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar algumas amostras\n",
    "for idx in [0, 1, 2]:\n",
    "    compare_predictions(exp3_val_dataset, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico comparativo de curvas de aprendizado\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history['val_loss'], label='Exp1 (Baseline)', linewidth=2)\n",
    "axes[0, 0].plot(exp2_history['val_loss'], label='Exp2 (CLAHE+DS)', linewidth=2)\n",
    "axes[0, 0].plot(exp3_history['val_loss'], label='Exp3 (Attention)', linewidth=2)\n",
    "axes[0, 0].set_title('Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_xlabel('Época')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# IoU\n",
    "axes[0, 1].plot(history['val_iou'], label='Exp1 (Baseline)', linewidth=2)\n",
    "axes[0, 1].plot(exp2_history['val_iou'], label='Exp2 (CLAHE+DS)', linewidth=2)\n",
    "axes[0, 1].plot(exp3_history['val_iou'], label='Exp3 (Attention)', linewidth=2)\n",
    "axes[0, 1].set_title('Validation IoU')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_xlabel('Época')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Dice\n",
    "axes[0, 2].plot(history['val_dice'], label='Exp1 (Baseline)', linewidth=2)\n",
    "axes[0, 2].plot(exp2_history['val_dice'], label='Exp2 (CLAHE+DS)', linewidth=2)\n",
    "axes[0, 2].plot(exp3_history['val_dice'], label='Exp3 (Attention)', linewidth=2)\n",
    "axes[0, 2].set_title('Validation Dice')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].set_xlabel('Época')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Barplot comparativo\n",
    "experiments = ['Exp1\\n(Baseline)', 'Exp2\\n(CLAHE+DS)', 'Exp2\\n(+TTA)', 'Exp3\\n(Attention)', 'Exp3\\n(+TTA)']\n",
    "dice_scores = [np.mean(all_dice), np.mean(all_dice_no_tta), np.mean(all_dice_tta), \n",
    "               np.mean(exp3_dice_no_tta), np.mean(exp3_dice_tta)]\n",
    "iou_scores = [np.mean(all_iou), np.mean(all_iou_no_tta), np.mean(all_iou_tta),\n",
    "              np.mean(exp3_iou_no_tta), np.mean(exp3_iou_tta)]\n",
    "\n",
    "x = np.arange(len(experiments))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 0].bar(x - width/2, dice_scores, width, label='Dice', color='steelblue')\n",
    "axes[1, 0].bar(x + width/2, iou_scores, width, label='IoU', color='coral')\n",
    "axes[1, 0].set_ylabel('Score')\n",
    "axes[1, 0].set_title('Comparação Final - Métricas')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(experiments)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "axes[1, 0].set_ylim([0.8, 1.0])\n",
    "\n",
    "# Boxplot Dice\n",
    "dice_data = [all_dice, all_dice_no_tta, all_dice_tta, exp3_dice_no_tta, exp3_dice_tta]\n",
    "bp = axes[1, 1].boxplot(dice_data, labels=['Exp1', 'Exp2', 'Exp2+TTA', 'Exp3', 'Exp3+TTA'])\n",
    "axes[1, 1].set_ylabel('Dice Score')\n",
    "axes[1, 1].set_title('Distribuição do Dice Score')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Boxplot IoU\n",
    "iou_data = [all_iou, all_iou_no_tta, all_iou_tta, exp3_iou_no_tta, exp3_iou_tta]\n",
    "bp = axes[1, 2].boxplot(iou_data, labels=['Exp1', 'Exp2', 'Exp2+TTA', 'Exp3', 'Exp3+TTA'])\n",
    "axes[1, 2].set_ylabel('IoU')\n",
    "axes[1, 2].set_title('Distribuição do IoU')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Comparação dos 3 Experimentos', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"                    COMPARAÇÃO FINAL DOS EXPERIMENTOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Experimento 1 (Baseline): U-Net + ResNet50\")\n",
    "print(\"-\"*80)\n",
    "print(f\"  IoU:  {np.mean(all_iou):.4f} +/- {np.std(all_iou):.4f}\")\n",
    "print(f\"  Dice: {np.mean(all_dice):.4f} +/- {np.std(all_dice):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Experimento 2: CLAHE + Data Aug Avançado + Deep Supervision\")\n",
    "print(\"-\"*80)\n",
    "print(\"  Sem TTA:\")\n",
    "print(f\"    IoU:  {np.mean(all_iou_no_tta):.4f} +/- {np.std(all_iou_no_tta):.4f}\")\n",
    "print(f\"    Dice: {np.mean(all_dice_no_tta):.4f} +/- {np.std(all_dice_no_tta):.4f}\")\n",
    "print(\"  Com TTA:\")\n",
    "print(f\"    IoU:  {np.mean(all_iou_tta):.4f} +/- {np.std(all_iou_tta):.4f}\")\n",
    "print(f\"    Dice: {np.mean(all_dice_tta):.4f} +/- {np.std(all_dice_tta):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Experimento 3: Attention U-Net (ASPP + Attention Gates + SE Blocks)\")\n",
    "print(\"-\"*80)\n",
    "print(\"  Sem TTA:\")\n",
    "print(f\"    IoU:  {np.mean(exp3_iou_no_tta):.4f} +/- {np.std(exp3_iou_no_tta):.4f}\")\n",
    "print(f\"    Dice: {np.mean(exp3_dice_no_tta):.4f} +/- {np.std(exp3_dice_no_tta):.4f}\")\n",
    "print(\"  Com TTA:\")\n",
    "print(f\"    IoU:  {np.mean(exp3_iou_tta):.4f} +/- {np.std(exp3_iou_tta):.4f}\")\n",
    "print(f\"    Dice: {np.mean(exp3_dice_tta):.4f} +/- {np.std(exp3_dice_tta):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"                              MELHORIAS\")\n",
    "print(\"=\"*80)\n",
    "baseline_dice = np.mean(all_dice)\n",
    "exp2_dice = np.mean(all_dice_tta)\n",
    "exp3_dice = np.mean(exp3_dice_tta)\n",
    "\n",
    "print(f\"\\nMelhoria Exp2 vs Baseline: {(exp2_dice - baseline_dice)*100:+.2f}% Dice\")\n",
    "print(f\"Melhoria Exp3 vs Baseline: {(exp3_dice - baseline_dice)*100:+.2f}% Dice\")\n",
    "print(f\"Melhoria Exp3 vs Exp2:     {(exp3_dice - exp2_dice)*100:+.2f}% Dice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# COMPARAÇÃO FINAL DOS EXPERIMENTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(exp3_history['train_loss'], label='Treino')\n",
    "axes[0].plot(exp3_history['val_loss'], label='Validação')\n",
    "axes[0].set_title('Exp3 - Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(exp3_history['train_iou'], label='Treino')\n",
    "axes[1].plot(exp3_history['val_iou'], label='Validação')\n",
    "axes[1].set_title('Exp3 - IoU')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot(exp3_history['train_dice'], label='Treino')\n",
    "axes[2].plot(exp3_history['val_dice'], label='Validação')\n",
    "axes[2].set_title('Exp3 - Dice Score')\n",
    "axes[2].legend()\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Época')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Experimento 3: Attention U-Net', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp3.7 - Gráficos de Treinamento Experimento 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar melhor modelo Exp3\n",
    "exp3_model.load_state_dict(torch.load('best_exp3_attention_unet.pth'))\n",
    "exp3_model.eval()\n",
    "\n",
    "# Criar TTA para Exp3\n",
    "tta_exp3 = TestTimeAugmentation(exp3_model, device)\n",
    "\n",
    "# Avaliação\n",
    "exp3_iou_no_tta = []\n",
    "exp3_dice_no_tta = []\n",
    "exp3_iou_tta = []\n",
    "exp3_dice_tta = []\n",
    "\n",
    "print(\"Avaliando Experimento 3 (com e sem TTA)...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(exp3_val_loader, desc='Avaliando Exp3'):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        for i in range(images.shape[0]):\n",
    "            img = images[i:i+1]\n",
    "            mask = masks[i:i+1]\n",
    "            \n",
    "            # Sem TTA\n",
    "            pred = torch.sigmoid(exp3_model(img))\n",
    "            pred_bin = (pred > 0.5).float()\n",
    "            \n",
    "            intersection = (pred_bin * mask).sum()\n",
    "            union = pred_bin.sum() + mask.sum() - intersection\n",
    "            iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "            dice = (2 * intersection + 1e-6) / (pred_bin.sum() + mask.sum() + 1e-6)\n",
    "            \n",
    "            exp3_iou_no_tta.append(iou.item())\n",
    "            exp3_dice_no_tta.append(dice.item())\n",
    "            \n",
    "            # Com TTA\n",
    "            pred_tta = tta_exp3(img)\n",
    "            pred_bin_tta = (pred_tta > 0.5).float()\n",
    "            \n",
    "            intersection = (pred_bin_tta * mask).sum()\n",
    "            union = pred_bin_tta.sum() + mask.sum() - intersection\n",
    "            iou_tta = (intersection + 1e-6) / (union + 1e-6)\n",
    "            dice_tta = (2 * intersection + 1e-6) / (pred_bin_tta.sum() + mask.sum() + 1e-6)\n",
    "            \n",
    "            exp3_iou_tta.append(iou_tta.item())\n",
    "            exp3_dice_tta.append(dice_tta.item())\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('=== RESULTADOS EXPERIMENTO 3 (Attention U-Net) ===')\n",
    "print('='*60)\n",
    "print('\\nSem TTA:')\n",
    "print(f'  IoU  - Média: {np.mean(exp3_iou_no_tta):.4f} | Std: {np.std(exp3_iou_no_tta):.4f}')\n",
    "print(f'  Dice - Média: {np.mean(exp3_dice_no_tta):.4f} | Std: {np.std(exp3_dice_no_tta):.4f}')\n",
    "print('\\nCom TTA:')\n",
    "print(f'  IoU  - Média: {np.mean(exp3_iou_tta):.4f} | Std: {np.std(exp3_iou_tta):.4f}')\n",
    "print(f'  Dice - Média: {np.mean(exp3_dice_tta):.4f} | Std: {np.std(exp3_dice_tta):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp3.6 - Avaliação Final do Experimento 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop de treinamento Experimento 3\n",
    "exp3_history = {'train_loss': [], 'val_loss': [], 'train_iou': [], 'val_iou': [], \n",
    "                'train_dice': [], 'val_dice': []}\n",
    "exp3_best_dice = 0\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INICIANDO TREINAMENTO - EXPERIMENTO 3 (Attention U-Net)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(EXP3_NUM_EPOCHS):\n",
    "    print(f'\\nEpoch {epoch+1}/{EXP3_NUM_EPOCHS}')\n",
    "    \n",
    "    train_loss, train_iou, train_dice = train_epoch_exp3(\n",
    "        exp3_model, exp3_train_loader, exp3_criterion, exp3_optimizer, exp3_scheduler\n",
    "    )\n",
    "    val_loss, val_iou, val_dice = validate_exp3(\n",
    "        exp3_model, exp3_val_loader, exp3_criterion\n",
    "    )\n",
    "    \n",
    "    exp3_history['train_loss'].append(train_loss)\n",
    "    exp3_history['val_loss'].append(val_loss)\n",
    "    exp3_history['train_iou'].append(train_iou)\n",
    "    exp3_history['val_iou'].append(val_iou)\n",
    "    exp3_history['train_dice'].append(train_dice)\n",
    "    exp3_history['val_dice'].append(val_dice)\n",
    "    \n",
    "    current_lr = exp3_optimizer.param_groups[1]['lr']\n",
    "    print(f'Train - Loss: {train_loss:.4f} | IoU: {train_iou:.4f} | Dice: {train_dice:.4f}')\n",
    "    print(f'Val   - Loss: {val_loss:.4f} | IoU: {val_iou:.4f} | Dice: {val_dice:.4f}')\n",
    "    print(f'LR (decoder): {current_lr:.2e}')\n",
    "    \n",
    "    if val_dice > exp3_best_dice:\n",
    "        exp3_best_dice = val_dice\n",
    "        torch.save(exp3_model.state_dict(), 'best_exp3_attention_unet.pth')\n",
    "        print(f'*** Modelo Exp3 salvo! Dice: {exp3_best_dice:.4f} ***')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"EXPERIMENTO 3 CONCLUÍDO - Melhor Dice: {exp3_best_dice:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_exp3(model, loader, criterion, optimizer, scheduler):\n",
    "    \"\"\"Treino do Experimento 3 com OneCycleLR\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "    \n",
    "    for images, masks in tqdm(loader, desc='Train Exp3'):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()  # OneCycleLR atualiza a cada batch\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        iou, dice = calc_metrics(outputs, masks)\n",
    "        total_iou += iou\n",
    "        total_dice += dice\n",
    "    \n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_iou/n, total_dice/n\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_exp3(model, loader, criterion):\n",
    "    \"\"\"Validação do Experimento 3\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "    \n",
    "    for images, masks in tqdm(loader, desc='Val Exp3'):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        iou, dice = calc_metrics(outputs, masks)\n",
    "        total_iou += iou\n",
    "        total_dice += dice\n",
    "    \n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_iou/n, total_dice/n\n",
    "\n",
    "print(\"Funções de treino Exp3 definidas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss e otimizador para Exp3\n",
    "exp3_dice_loss = smp.losses.DiceLoss(mode='binary')\n",
    "exp3_bce_loss = smp.losses.SoftBCEWithLogitsLoss()\n",
    "# Adicionar Focal Loss para lidar com desbalanceamento\n",
    "exp3_focal_loss = smp.losses.FocalLoss(mode='binary', alpha=0.25, gamma=2.0)\n",
    "\n",
    "def exp3_criterion(pred, target):\n",
    "    \"\"\"Loss combinada: BCE + Dice + Focal\"\"\"\n",
    "    return 0.4 * exp3_bce_loss(pred, target) + 0.4 * exp3_dice_loss(pred, target) + 0.2 * exp3_focal_loss(pred, target)\n",
    "\n",
    "# Otimizador com weight decay diferenciado\n",
    "encoder_params = list(exp3_model.encoder.parameters())\n",
    "decoder_params = [p for n, p in exp3_model.named_parameters() if 'encoder' not in n]\n",
    "\n",
    "exp3_optimizer = optim.AdamW([\n",
    "    {'params': encoder_params, 'lr': EXP3_LEARNING_RATE * 0.1},  # Encoder: LR menor\n",
    "    {'params': decoder_params, 'lr': EXP3_LEARNING_RATE}         # Decoder: LR normal\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "exp3_scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    exp3_optimizer,\n",
    "    max_lr=[EXP3_LEARNING_RATE * 0.1, EXP3_LEARNING_RATE],\n",
    "    epochs=EXP3_NUM_EPOCHS,\n",
    "    steps_per_epoch=len(exp3_train_loader),\n",
    "    pct_start=0.1,\n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "print(\"Otimizador e Loss do Exp3 configurados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp3.5 - Treinamento do Experimento 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar os mesmos transforms do Exp2 (com CLAHE) para comparação justa\n",
    "exp3_train_dataset = OpticDiscDataset(train_pairs, get_exp2_train_transforms())\n",
    "exp3_val_dataset = OpticDiscDataset(val_pairs, get_exp2_val_transforms())\n",
    "\n",
    "exp3_train_loader = DataLoader(exp3_train_dataset, batch_size=EXP3_BATCH_SIZE, \n",
    "                                shuffle=True, num_workers=2, pin_memory=True)\n",
    "exp3_val_loader = DataLoader(exp3_val_dataset, batch_size=EXP3_BATCH_SIZE, \n",
    "                              shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Criar modelo Attention U-Net\n",
    "exp3_model = AttentionUNet(\n",
    "    encoder_name=EXP3_ENCODER,\n",
    "    encoder_weights=EXP3_ENCODER_WEIGHTS,\n",
    "    in_channels=3,\n",
    "    classes=1\n",
    ").to(device)\n",
    "\n",
    "# Contar parâmetros\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Exp3 - Modelo: Attention U-Net')\n",
    "print(f'Exp3 - Parâmetros treináveis: {count_parameters(exp3_model):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp3.4 - Dataset e Modelo do Experimento 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention U-Net com:\n",
    "    - Encoder ResNet50 pré-treinado\n",
    "    - Attention Gates nas skip connections\n",
    "    - SE blocks no decoder\n",
    "    - ASPP no bottleneck\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_name='resnet50', encoder_weights='imagenet', \n",
    "                 in_channels=3, classes=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder pré-treinado\n",
    "        self.encoder = smp.encoders.get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=5,\n",
    "            weights=encoder_weights\n",
    "        )\n",
    "        \n",
    "        # Canais do encoder ResNet50: [3, 64, 256, 512, 1024, 2048]\n",
    "        encoder_channels = self.encoder.out_channels\n",
    "        \n",
    "        # ASPP no bottleneck\n",
    "        self.aspp = ASPP(encoder_channels[-1], encoder_channels[-1] // 2)\n",
    "        \n",
    "        # Decoder channels\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        \n",
    "        # Attention Gates\n",
    "        self.attention_gates = nn.ModuleList([\n",
    "            AttentionGate(decoder_channels[0], encoder_channels[-2], decoder_channels[0] // 2),  # 1024 -> 256\n",
    "            AttentionGate(decoder_channels[1], encoder_channels[-3], decoder_channels[1] // 2),  # 512 -> 128\n",
    "            AttentionGate(decoder_channels[2], encoder_channels[-4], decoder_channels[2] // 2),  # 256 -> 64\n",
    "            AttentionGate(decoder_channels[3], encoder_channels[-5], decoder_channels[3] // 2),  # 64 -> 32\n",
    "        ])\n",
    "        \n",
    "        # Decoder blocks com SE\n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "        \n",
    "        # Block 1: ASPP output + skip4\n",
    "        in_ch = encoder_channels[-1] // 2 + encoder_channels[-2]  # aspp + skip\n",
    "        self.decoder_blocks.append(self._make_decoder_block(in_ch, decoder_channels[0]))\n",
    "        \n",
    "        # Block 2-4\n",
    "        for i in range(1, 4):\n",
    "            in_ch = decoder_channels[i-1] + encoder_channels[-2-i]\n",
    "            self.decoder_blocks.append(self._make_decoder_block(in_ch, decoder_channels[i]))\n",
    "        \n",
    "        # Block 5 (sem skip)\n",
    "        self.decoder_blocks.append(self._make_decoder_block(decoder_channels[3], decoder_channels[4]))\n",
    "        \n",
    "        # Segmentation head\n",
    "        self.segmentation_head = nn.Conv2d(decoder_channels[-1], classes, kernel_size=1)\n",
    "    \n",
    "    def _make_decoder_block(self, in_channels, out_channels):\n",
    "        \"\"\"Cria um bloco do decoder com SE block\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SqueezeExcitation(out_channels, reduction=16)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        features = self.encoder(x)\n",
    "        # features: [input, stage1, stage2, stage3, stage4, stage5]\n",
    "        # Para ResNet50: [3, 64, 256, 512, 1024, 2048]\n",
    "        \n",
    "        # ASPP no bottleneck\n",
    "        x = self.aspp(features[-1])\n",
    "        \n",
    "        # Decoder com Attention Gates\n",
    "        skips = features[:-1][::-1]  # Inverter ordem dos skips\n",
    "        \n",
    "        for i in range(4):\n",
    "            # Upsample\n",
    "            x = nn.functional.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # Attention Gate na skip connection\n",
    "            skip = self.attention_gates[i](x, skips[i])\n",
    "            \n",
    "            # Concatenar\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            \n",
    "            # Decoder block\n",
    "            x = self.decoder_blocks[i](x)\n",
    "        \n",
    "        # Último upsample (sem skip)\n",
    "        x = nn.functional.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        x = self.decoder_blocks[4](x)\n",
    "        \n",
    "        # Segmentation head\n",
    "        x = self.segmentation_head(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"Attention U-Net definida!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp3.3 - Attention U-Net Completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionGate(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Gate: Permite ao modelo focar nas regiões relevantes\n",
    "    nas skip connections, suprimindo respostas irrelevantes.\n",
    "    \n",
    "    Referência: \"Attention U-Net: Learning Where to Look for the Pancreas\"\n",
    "    \"\"\"\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        \"\"\"\n",
    "        F_g: número de canais do gating signal (do decoder)\n",
    "        F_l: número de canais do skip connection (do encoder)\n",
    "        F_int: número de canais intermediários\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        \n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, g, x):\n",
    "        \"\"\"\n",
    "        g: gating signal do decoder (menor resolução)\n",
    "        x: skip connection do encoder (maior resolução)\n",
    "        \"\"\"\n",
    "        # Redimensionar g para o tamanho de x se necessário\n",
    "        if g.shape[2:] != x.shape[2:]:\n",
    "            g = nn.functional.interpolate(g, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        \n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        \n",
    "        return x * psi\n",
    "\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation Block: Recalibração adaptativa de canais\n",
    "    Aprende a importância relativa de cada canal de features\n",
    "    \n",
    "    Referência: \"Squeeze-and-Excitation Networks\"\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.shape\n",
    "        \n",
    "        # Squeeze: Global Average Pooling\n",
    "        y = self.squeeze(x).view(b, c)\n",
    "        \n",
    "        # Excitation: FC layers\n",
    "        y = self.excitation(y).view(b, c, 1, 1)\n",
    "        \n",
    "        # Scale\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    \"\"\"\n",
    "    Atrous Spatial Pyramid Pooling: Captura contexto multi-escala\n",
    "    usando convoluções dilatadas com diferentes taxas de dilatação\n",
    "    \n",
    "    Referência: \"DeepLab: Semantic Image Segmentation\"\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, rates=[6, 12, 18]):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1x1 convolution\n",
    "        self.conv1x1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Atrous convolutions com diferentes rates\n",
    "        self.atrous_convs = nn.ModuleList()\n",
    "        for rate in rates:\n",
    "            self.atrous_convs.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False),\n",
    "                    nn.BatchNorm2d(out_channels),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        self.global_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Projeção final\n",
    "        num_features = out_channels * (2 + len(rates))  # 1x1 + atrous + global\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(num_features, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        size = x.shape[2:]\n",
    "        \n",
    "        features = [self.conv1x1(x)]\n",
    "        \n",
    "        for atrous_conv in self.atrous_convs:\n",
    "            features.append(atrous_conv(x))\n",
    "        \n",
    "        # Global pooling feature\n",
    "        global_feat = self.global_pool(x)\n",
    "        global_feat = nn.functional.interpolate(global_feat, size=size, mode='bilinear', align_corners=False)\n",
    "        features.append(global_feat)\n",
    "        \n",
    "        # Concatenar e projetar\n",
    "        x = torch.cat(features, dim=1)\n",
    "        x = self.project(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"Módulos de Atenção definidos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp3.2 - Módulos de Atenção (Attention Gates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXPERIMENTO 3 - Configurações\n",
    "# ============================================\n",
    "\n",
    "EXP3_BATCH_SIZE = 8\n",
    "EXP3_NUM_EPOCHS = 50\n",
    "EXP3_LEARNING_RATE = 1e-4\n",
    "EXP3_IMG_SIZE = 512\n",
    "\n",
    "EXP3_ENCODER = 'resnet50'\n",
    "EXP3_ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "print(\"=== Experimento 3: Attention U-Net com Modificação na Arquitetura ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp3.1 - Configurações do Experimento 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EXPERIMENTO 3: Attention U-Net (Modificação na Arquitetura)\n",
    "\n",
    "**Modificação na arquitetura da rede neural:**\n",
    "1. **Attention Gates**: Mecanismos de atenção nas skip connections que permitem ao modelo focar nas regiões relevantes\n",
    "2. **Squeeze-and-Excitation (SE) Blocks**: Recalibração adaptativa dos canais de features\n",
    "3. **ASPP (Atrous Spatial Pyramid Pooling)**: Captura de contexto multi-escala no bottleneck\n",
    "\n",
    "Esta é uma modificação substancial na topologia da rede, não apenas um aumento de largura/profundidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(exp2_history['train_loss'], label='Treino')\n",
    "axes[0].plot(exp2_history['val_loss'], label='Validação')\n",
    "axes[0].set_title('Exp2 - Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(exp2_history['train_iou'], label='Treino')\n",
    "axes[1].plot(exp2_history['val_iou'], label='Validação')\n",
    "axes[1].set_title('Exp2 - IoU')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot(exp2_history['train_dice'], label='Treino')\n",
    "axes[2].plot(exp2_history['val_dice'], label='Validação')\n",
    "axes[2].set_title('Exp2 - Dice Score')\n",
    "axes[2].legend()\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Época')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Experimento 2: Pré-processamento + Data Aug + Deep Supervision', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp2.9 - Gráficos de Treinamento Experimento 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar melhor modelo e avaliar com TTA\n",
    "exp2_model.load_state_dict(torch.load('best_exp2_model.pth'))\n",
    "exp2_model.eval()\n",
    "\n",
    "# Criar TTA\n",
    "tta = TestTimeAugmentation(exp2_model, device)\n",
    "\n",
    "# Avaliação sem TTA\n",
    "all_iou_no_tta = []\n",
    "all_dice_no_tta = []\n",
    "\n",
    "# Avaliação com TTA\n",
    "all_iou_tta = []\n",
    "all_dice_tta = []\n",
    "\n",
    "print(\"Avaliando Experimento 2 (com e sem TTA)...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(exp2_val_loader, desc='Avaliando'):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        for i in range(images.shape[0]):\n",
    "            img = images[i:i+1]\n",
    "            mask = masks[i:i+1]\n",
    "            \n",
    "            # Sem TTA\n",
    "            pred_no_tta = torch.sigmoid(exp2_model(img))\n",
    "            pred_bin_no_tta = (pred_no_tta > 0.5).float()\n",
    "            \n",
    "            intersection = (pred_bin_no_tta * mask).sum()\n",
    "            union = pred_bin_no_tta.sum() + mask.sum() - intersection\n",
    "            iou_no_tta = (intersection + 1e-6) / (union + 1e-6)\n",
    "            dice_no_tta = (2 * intersection + 1e-6) / (pred_bin_no_tta.sum() + mask.sum() + 1e-6)\n",
    "            \n",
    "            all_iou_no_tta.append(iou_no_tta.item())\n",
    "            all_dice_no_tta.append(dice_no_tta.item())\n",
    "            \n",
    "            # Com TTA\n",
    "            pred_tta = tta(img)\n",
    "            pred_bin_tta = (pred_tta > 0.5).float()\n",
    "            \n",
    "            intersection = (pred_bin_tta * mask).sum()\n",
    "            union = pred_bin_tta.sum() + mask.sum() - intersection\n",
    "            iou_tta = (intersection + 1e-6) / (union + 1e-6)\n",
    "            dice_tta = (2 * intersection + 1e-6) / (pred_bin_tta.sum() + mask.sum() + 1e-6)\n",
    "            \n",
    "            all_iou_tta.append(iou_tta.item())\n",
    "            all_dice_tta.append(dice_tta.item())\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('=== RESULTADOS EXPERIMENTO 2 ===')\n",
    "print('='*60)\n",
    "print('\\nSem TTA:')\n",
    "print(f'  IoU  - Média: {np.mean(all_iou_no_tta):.4f} | Std: {np.std(all_iou_no_tta):.4f}')\n",
    "print(f'  Dice - Média: {np.mean(all_dice_no_tta):.4f} | Std: {np.std(all_dice_no_tta):.4f}')\n",
    "print('\\nCom TTA (7 augmentations):')\n",
    "print(f'  IoU  - Média: {np.mean(all_iou_tta):.4f} | Std: {np.std(all_iou_tta):.4f}')\n",
    "print(f'  Dice - Média: {np.mean(all_dice_tta):.4f} | Std: {np.std(all_dice_tta):.4f}')\n",
    "print('\\nMelhoria com TTA:')\n",
    "print(f'  IoU:  +{(np.mean(all_iou_tta) - np.mean(all_iou_no_tta))*100:.2f}%')\n",
    "print(f'  Dice: +{(np.mean(all_dice_tta) - np.mean(all_dice_no_tta))*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp2.8 - Avaliação Final com TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop de treinamento Experimento 2\n",
    "exp2_history = {'train_loss': [], 'val_loss': [], 'train_iou': [], 'val_iou': [], \n",
    "                'train_dice': [], 'val_dice': []}\n",
    "exp2_best_dice = 0\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INICIANDO TREINAMENTO - EXPERIMENTO 2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(EXP2_NUM_EPOCHS):\n",
    "    print(f'\\nEpoch {epoch+1}/{EXP2_NUM_EPOCHS}')\n",
    "    \n",
    "    train_loss, train_iou, train_dice = train_epoch_exp2(\n",
    "        exp2_model, exp2_train_loader, exp2_criterion, exp2_optimizer\n",
    "    )\n",
    "    val_loss, val_iou, val_dice = validate_exp2(\n",
    "        exp2_model, exp2_val_loader, exp2_criterion\n",
    "    )\n",
    "    exp2_scheduler.step()\n",
    "    \n",
    "    exp2_history['train_loss'].append(train_loss)\n",
    "    exp2_history['val_loss'].append(val_loss)\n",
    "    exp2_history['train_iou'].append(train_iou)\n",
    "    exp2_history['val_iou'].append(val_iou)\n",
    "    exp2_history['train_dice'].append(train_dice)\n",
    "    exp2_history['val_dice'].append(val_dice)\n",
    "    \n",
    "    print(f'Train - Loss: {train_loss:.4f} | IoU: {train_iou:.4f} | Dice: {train_dice:.4f}')\n",
    "    print(f'Val   - Loss: {val_loss:.4f} | IoU: {val_iou:.4f} | Dice: {val_dice:.4f}')\n",
    "    print(f'LR: {exp2_scheduler.get_last_lr()[0]:.2e}')\n",
    "    \n",
    "    if val_dice > exp2_best_dice:\n",
    "        exp2_best_dice = val_dice\n",
    "        torch.save(exp2_model.state_dict(), 'best_exp2_model.pth')\n",
    "        print(f'*** Modelo Exp2 salvo! Dice: {exp2_best_dice:.4f} ***')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"EXPERIMENTO 2 CONCLUÍDO - Melhor Dice: {exp2_best_dice:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_exp2(model, loader, criterion, optimizer):\n",
    "    \"\"\"Treino com Deep Supervision\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "    \n",
    "    for images, masks in tqdm(loader, desc='Train Exp2'):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Loss com deep supervision\n",
    "        loss, main_output = deep_supervision_loss(outputs, masks, criterion)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping para estabilidade\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        iou, dice = calc_metrics(main_output, masks)\n",
    "        total_iou += iou\n",
    "        total_dice += dice\n",
    "    \n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_iou/n, total_dice/n\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_exp2(model, loader, criterion):\n",
    "    \"\"\"Validação do Experimento 2\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "    \n",
    "    for images, masks in tqdm(loader, desc='Val Exp2'):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # No eval, deep supervision retorna apenas saída principal\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        iou, dice = calc_metrics(outputs, masks)\n",
    "        total_iou += iou\n",
    "        total_dice += dice\n",
    "    \n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_iou/n, total_dice/n\n",
    "\n",
    "print(\"Funções de treino Exp2 definidas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar modelo com Deep Supervision\n",
    "exp2_model = UNetWithDeepSupervision(\n",
    "    encoder_name=EXP2_ENCODER,\n",
    "    encoder_weights=EXP2_ENCODER_WEIGHTS,\n",
    "    in_channels=3,\n",
    "    classes=1\n",
    ").to(device)\n",
    "\n",
    "# Loss e otimizador\n",
    "exp2_dice_loss = smp.losses.DiceLoss(mode='binary')\n",
    "exp2_bce_loss = smp.losses.SoftBCEWithLogitsLoss()\n",
    "\n",
    "def exp2_criterion(pred, target):\n",
    "    return 0.5 * exp2_bce_loss(pred, target) + 0.5 * exp2_dice_loss(pred, target)\n",
    "\n",
    "exp2_optimizer = optim.AdamW(exp2_model.parameters(), lr=EXP2_LEARNING_RATE, weight_decay=1e-4)\n",
    "exp2_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    exp2_optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    ")\n",
    "\n",
    "print(f'Exp2 - Modelo: U-Net com Deep Supervision')\n",
    "print(f'Exp2 - Encoder: {EXP2_ENCODER}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp2.7 - Modelo e Treinamento do Experimento 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_clahe_effect(pairs, idx=0):\n",
    "    \"\"\"Compara imagem original vs com CLAHE\"\"\"\n",
    "    pair = pairs[idx]\n",
    "    \n",
    "    # Carregar imagem original\n",
    "    img_original = np.array(Image.open(pair['image']).convert('RGB'))\n",
    "    \n",
    "    # Aplicar CLAHE\n",
    "    img_clahe = apply_clahe_preprocessing(img_original)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    axes[0].imshow(img_original)\n",
    "    axes[0].set_title('Imagem Original')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(img_clahe)\n",
    "    axes[1].set_title('Com CLAHE (Contraste Realçado)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Efeito do Pré-processamento CLAHE', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar efeito CLAHE\n",
    "compare_clahe_effect(pairs, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp2.6 - Visualizar efeito do pré-processamento CLAHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar datasets com novos transforms\n",
    "exp2_train_dataset = OpticDiscDataset(train_pairs, get_exp2_train_transforms())\n",
    "exp2_val_dataset = OpticDiscDataset(val_pairs, get_exp2_val_transforms())\n",
    "\n",
    "exp2_train_loader = DataLoader(exp2_train_dataset, batch_size=EXP2_BATCH_SIZE, \n",
    "                                shuffle=True, num_workers=2, pin_memory=True)\n",
    "exp2_val_loader = DataLoader(exp2_val_dataset, batch_size=EXP2_BATCH_SIZE, \n",
    "                              shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f'Exp2 - Treino: {len(exp2_train_dataset)} | Validação: {len(exp2_val_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp2.5 - Dataset e DataLoaders do Experimento 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTimeAugmentation:\n",
    "    \"\"\"\n",
    "    Test Time Augmentation: faz múltiplas predições com diferentes \n",
    "    augmentations e combina os resultados para predição mais robusta\n",
    "    \"\"\"\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        \n",
    "    def __call__(self, image):\n",
    "        \"\"\"\n",
    "        image: tensor normalizado (1, C, H, W)\n",
    "        retorna: média das predições (1, 1, H, W)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Original\n",
    "            pred = torch.sigmoid(self.model(image))\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            # Flip horizontal\n",
    "            flipped_h = torch.flip(image, dims=[3])\n",
    "            pred_h = torch.sigmoid(self.model(flipped_h))\n",
    "            pred_h = torch.flip(pred_h, dims=[3])\n",
    "            predictions.append(pred_h)\n",
    "            \n",
    "            # Flip vertical\n",
    "            flipped_v = torch.flip(image, dims=[2])\n",
    "            pred_v = torch.sigmoid(self.model(flipped_v))\n",
    "            pred_v = torch.flip(pred_v, dims=[2])\n",
    "            predictions.append(pred_v)\n",
    "            \n",
    "            # Flip ambos\n",
    "            flipped_hv = torch.flip(image, dims=[2, 3])\n",
    "            pred_hv = torch.sigmoid(self.model(flipped_hv))\n",
    "            pred_hv = torch.flip(pred_hv, dims=[2, 3])\n",
    "            predictions.append(pred_hv)\n",
    "            \n",
    "            # Rotação 90°\n",
    "            rotated_90 = torch.rot90(image, k=1, dims=[2, 3])\n",
    "            pred_90 = torch.sigmoid(self.model(rotated_90))\n",
    "            pred_90 = torch.rot90(pred_90, k=-1, dims=[2, 3])\n",
    "            predictions.append(pred_90)\n",
    "            \n",
    "            # Rotação 180°\n",
    "            rotated_180 = torch.rot90(image, k=2, dims=[2, 3])\n",
    "            pred_180 = torch.sigmoid(self.model(rotated_180))\n",
    "            pred_180 = torch.rot90(pred_180, k=-2, dims=[2, 3])\n",
    "            predictions.append(pred_180)\n",
    "            \n",
    "            # Rotação 270°\n",
    "            rotated_270 = torch.rot90(image, k=3, dims=[2, 3])\n",
    "            pred_270 = torch.sigmoid(self.model(rotated_270))\n",
    "            pred_270 = torch.rot90(pred_270, k=-3, dims=[2, 3])\n",
    "            predictions.append(pred_270)\n",
    "        \n",
    "        # Média de todas as predições\n",
    "        avg_prediction = torch.stack(predictions).mean(dim=0)\n",
    "        return avg_prediction\n",
    "\n",
    "print(\"TTA definido!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp2.4 - Test Time Augmentation (TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetWithDeepSupervision(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net com Deep Supervision: adiciona saídas auxiliares em diferentes \n",
    "    escalas do decoder para melhorar o fluxo de gradiente e convergência\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_name='resnet50', encoder_weights='imagenet', \n",
    "                 in_channels=3, classes=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Modelo base U-Net\n",
    "        self.base_model = smp.Unet(\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights=encoder_weights,\n",
    "            in_channels=in_channels,\n",
    "            classes=classes,\n",
    "            activation=None,\n",
    "            decoder_channels=(256, 128, 64, 32, 16)\n",
    "        )\n",
    "        \n",
    "        # Cabeças de Deep Supervision para diferentes escalas\n",
    "        # Saídas auxiliares nos estágios intermediários do decoder\n",
    "        self.ds_head_1 = nn.Conv2d(32, classes, kernel_size=1)   # 1/4 da resolução\n",
    "        self.ds_head_2 = nn.Conv2d(64, classes, kernel_size=1)   # 1/8 da resolução\n",
    "        self.ds_head_3 = nn.Conv2d(128, classes, kernel_size=1)  # 1/16 da resolução\n",
    "        \n",
    "        self.training_mode = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        features = self.base_model.encoder(x)\n",
    "        \n",
    "        # Decoder com captura de features intermediários\n",
    "        decoder_output = features[0]  # features mais profundo\n",
    "        \n",
    "        decoder_blocks = self.base_model.decoder.blocks\n",
    "        \n",
    "        # Armazenar outputs intermediários\n",
    "        intermediate_outputs = []\n",
    "        \n",
    "        # Processar cada bloco do decoder\n",
    "        for i, block in enumerate(decoder_blocks):\n",
    "            skip = features[i + 1] if i + 1 < len(features) else None\n",
    "            decoder_output = block(decoder_output, skip)\n",
    "            \n",
    "            # Capturar outputs para deep supervision (nos estágios 2, 3, 4)\n",
    "            if i == 2:  # 1/8 resolução\n",
    "                intermediate_outputs.append(decoder_output)\n",
    "            elif i == 3:  # 1/4 resolução\n",
    "                intermediate_outputs.append(decoder_output)\n",
    "        \n",
    "        # Saída principal\n",
    "        main_output = self.base_model.segmentation_head(decoder_output)\n",
    "        \n",
    "        if self.training_mode and self.training:\n",
    "            # Saídas auxiliares (deep supervision)\n",
    "            ds_out_2 = self.ds_head_2(intermediate_outputs[0])  # 1/8\n",
    "            ds_out_1 = self.ds_head_1(intermediate_outputs[1])  # 1/4\n",
    "            \n",
    "            # Redimensionar para o tamanho original\n",
    "            target_size = main_output.shape[2:]\n",
    "            ds_out_1 = nn.functional.interpolate(ds_out_1, size=target_size, mode='bilinear', align_corners=False)\n",
    "            ds_out_2 = nn.functional.interpolate(ds_out_2, size=target_size, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            return main_output, ds_out_1, ds_out_2\n",
    "        \n",
    "        return main_output\n",
    "\n",
    "def deep_supervision_loss(outputs, target, criterion, weights=[1.0, 0.4, 0.2]):\n",
    "    \"\"\"\n",
    "    Calcula loss combinada para deep supervision\n",
    "    weights: pesos para [saída principal, ds1, ds2]\n",
    "    \"\"\"\n",
    "    if isinstance(outputs, tuple):\n",
    "        main_out, ds1, ds2 = outputs\n",
    "        loss = weights[0] * criterion(main_out, target)\n",
    "        loss += weights[1] * criterion(ds1, target)\n",
    "        loss += weights[2] * criterion(ds2, target)\n",
    "        return loss, main_out\n",
    "    else:\n",
    "        return criterion(outputs, target), outputs\n",
    "\n",
    "print(\"Deep Supervision definido!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp2.3 - Deep Supervision (Múltiplas Saídas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def apply_clahe_preprocessing(image):\n",
    "    \"\"\"\n",
    "    Aplica CLAHE (Contrast Limited Adaptive Histogram Equalization) \n",
    "    no canal de luminância para realçar estruturas em imagens de fundo de olho\n",
    "    \"\"\"\n",
    "    # Converter para LAB color space\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    \n",
    "    # Aplicar CLAHE no canal L (luminância)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    l_clahe = clahe.apply(l)\n",
    "    \n",
    "    # Recombinar canais\n",
    "    lab_clahe = cv2.merge([l_clahe, a, b])\n",
    "    \n",
    "    # Converter de volta para RGB\n",
    "    result = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2RGB)\n",
    "    return result\n",
    "\n",
    "def get_exp2_train_transforms():\n",
    "    \"\"\"\n",
    "    Data Augmentation avançado para imagens médicas:\n",
    "    - CLAHE como pré-processamento\n",
    "    - ElasticTransform para simular deformações anatômicas\n",
    "    - GridDistortion e OpticalDistortion\n",
    "    - CoarseDropout para regularização\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        # Pré-processamento CLAHE (sempre aplicado)\n",
    "        A.Lambda(image=apply_clahe_preprocessing),\n",
    "        \n",
    "        A.Resize(EXP2_IMG_SIZE, EXP2_IMG_SIZE),\n",
    "        \n",
    "        # Augmentations geométricos\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=45, p=0.5),\n",
    "        \n",
    "        # Deformações específicas para imagens médicas\n",
    "        A.OneOf([\n",
    "            A.ElasticTransform(alpha=120, sigma=120 * 0.05, p=1.0),\n",
    "            A.GridDistortion(num_steps=5, distort_limit=0.3, p=1.0),\n",
    "            A.OpticalDistortion(distort_limit=0.5, shift_limit=0.5, p=1.0),\n",
    "        ], p=0.4),\n",
    "        \n",
    "        # Augmentations de cor/intensidade\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(var_limit=(10, 50)),\n",
    "            A.GaussianBlur(blur_limit=3),\n",
    "            A.MedianBlur(blur_limit=3),\n",
    "            A.MotionBlur(blur_limit=3),\n",
    "        ], p=0.3),\n",
    "        \n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n",
    "            A.CLAHE(clip_limit=4),\n",
    "            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20),\n",
    "            A.RandomGamma(gamma_limit=(80, 120)),\n",
    "        ], p=0.4),\n",
    "        \n",
    "        # Regularização: CoarseDropout (simula oclusões)\n",
    "        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, \n",
    "                        min_holes=1, min_height=8, min_width=8,\n",
    "                        fill_value=0, p=0.3),\n",
    "        \n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_exp2_val_transforms():\n",
    "    \"\"\"Transforms de validação com CLAHE\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Lambda(image=apply_clahe_preprocessing),\n",
    "        A.Resize(EXP2_IMG_SIZE, EXP2_IMG_SIZE),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "print(\"Transforms do Experimento 2 definidos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp2.2 - Pré-processamento CLAHE e Data Augmentation Avançado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp2.1 - Configurações do Experimento 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentação do Disco Óptico\n",
    "\n",
    "Usando U-Net com ResNet pré-treinada para imagens médicas (segmentation_models_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalação e Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar bibliotecas necessárias\n",
    "!pip install segmentation-models-pytorch albumentations -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from skimage.draw import polygon\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Dispositivo: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== OPÇÃO 1: Montar Google Drive ====\n",
    "# (Se falhar, use a Opção 2 abaixo)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    ROOT_DIR = '/content/drive/MyDrive/PapilaDB/'\n",
    "    print(\"Drive montado!\")\n",
    "except:\n",
    "    print(\"Falha ao montar Drive. Use a Opção 2.\")\n",
    "    ROOT_DIR = '/content/PapilaDB/'\n",
    "\n",
    "# ==== OPÇÃO 2: Upload direto via ZIP ====\n",
    "# 1. Compacte a pasta PapilaDB em um arquivo .zip\n",
    "# 2. Descomente e execute as linhas abaixo:\n",
    "\n",
    "# from google.colab import files\n",
    "# import zipfile\n",
    "# uploaded = files.upload()  # Selecione o arquivo PapilaDB.zip\n",
    "# with zipfile.ZipFile('PapilaDB.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('/content/')\n",
    "# ROOT_DIR = '/content/PapilaDB/'\n",
    "\n",
    "# Verificar\n",
    "import os\n",
    "print(f\"ROOT_DIR: {ROOT_DIR}\")\n",
    "print(f\"Existe: {os.path.exists(ROOT_DIR)}\")\n",
    "if os.path.exists(ROOT_DIR):\n",
    "    print(f\"Conteúdo: {os.listdir(ROOT_DIR)}\")\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "IMG_SIZE = 512\n",
    "\n",
    "ENCODER = 'resnet50'\n",
    "ENCODER_WEIGHTS = 'imagenet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparar Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/weslley/Code/Glaucoma Diagnostico/PapilaDB/FundusImages/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-851607683.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcontour_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mROOT_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'ExpertsSegmentations/Contours/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimg_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcontour_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontour_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/weslley/Code/Glaucoma Diagnostico/PapilaDB/FundusImages/'"
     ]
    }
   ],
   "source": [
    "img_dir = ROOT_DIR + 'FundusImages/'\n",
    "contour_dir = ROOT_DIR + 'ExpertsSegmentations/Contours/'\n",
    "\n",
    "img_files = sorted(os.listdir(img_dir))\n",
    "contour_files = sorted(os.listdir(contour_dir))\n",
    "\n",
    "# Filtrar contornos de disco\n",
    "disc_contours = [f for f in contour_files if 'disc' in f.lower()]\n",
    "\n",
    "print(f'Imagens: {len(img_files)}')\n",
    "print(f'Contornos disco: {len(disc_contours)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar pares imagem-contorno\n",
    "def get_pairs():\n",
    "    pairs = []\n",
    "    for img_file in img_files:\n",
    "        img_id = os.path.splitext(img_file)[0]\n",
    "        for cont in disc_contours:\n",
    "            if img_id in cont:\n",
    "                pairs.append({\n",
    "                    'image': os.path.join(img_dir, img_file),\n",
    "                    'contour': os.path.join(contour_dir, cont)\n",
    "                })\n",
    "                break\n",
    "    return pairs\n",
    "\n",
    "pairs = get_pairs()\n",
    "print(f'Pares encontrados: {len(pairs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation (Albumentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose([\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(var_limit=(10, 50)),\n",
    "            A.GaussianBlur(blur_limit=3),\n",
    "            A.MedianBlur(blur_limit=3),\n",
    "        ], p=0.3),\n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2),\n",
    "            A.CLAHE(clip_limit=2),\n",
    "            A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10),\n",
    "        ], p=0.3),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_val_transforms():\n",
    "    return A.Compose([\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpticDiscDataset(Dataset):\n",
    "    def __init__(self, pairs, transforms=None):\n",
    "        self.pairs = pairs\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        # Carregar imagem\n",
    "        image = np.array(Image.open(pair['image']).convert('RGB'))\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        # Criar máscara do contorno\n",
    "        contour = np.loadtxt(pair['contour'])\n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        \n",
    "        rr, cc = polygon(contour[:, 1], contour[:, 0], mask.shape)\n",
    "        mask[rr, cc] = 1\n",
    "        \n",
    "        # Aplicar transformações\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "        \n",
    "        return image, mask.float().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dados\n",
    "train_pairs, val_pairs = train_test_split(pairs, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = OpticDiscDataset(train_pairs, get_train_transforms())\n",
    "val_dataset = OpticDiscDataset(val_pairs, get_val_transforms())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f'Treino: {len(train_dataset)} | Validação: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizar Amostras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(dataset, idx=0):\n",
    "    img, mask = dataset[idx]\n",
    "    \n",
    "    # Desnormalizar\n",
    "    img_np = img.numpy().transpose(1, 2, 0)\n",
    "    img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    ax[0].imshow(img_np)\n",
    "    ax[0].set_title('Imagem')\n",
    "    ax[1].imshow(mask.squeeze(), cmap='gray')\n",
    "    ax[1].set_title('Máscara')\n",
    "    \n",
    "    # Overlay\n",
    "    overlay = img_np.copy()\n",
    "    m = mask.squeeze().numpy()\n",
    "    overlay[m > 0.5] = overlay[m > 0.5] * 0.5 + np.array([0, 1, 0]) * 0.5\n",
    "    ax[2].imshow(overlay)\n",
    "    ax[2].set_title('Overlay')\n",
    "    \n",
    "    for a in ax: a.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_sample(train_dataset, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Modelo U-Net com ResNet (SMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar modelo usando segmentation_models_pytorch\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER,\n",
    "    encoder_weights=ENCODER_WEIGHTS,\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    "    activation=None  # Usaremos sigmoid na loss\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "print(f'Modelo: U-Net com encoder {ENCODER}')\n",
    "print(f'Pesos: {ENCODER_WEIGHTS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternativa: Usar outros modelos do SMP\n",
    "# model = smp.DeepLabV3Plus(encoder_name=ENCODER, encoder_weights=ENCODER_WEIGHTS, classes=1)\n",
    "# model = smp.FPN(encoder_name=ENCODER, encoder_weights=ENCODER_WEIGHTS, classes=1)\n",
    "# model = smp.PSPNet(encoder_name=ENCODER, encoder_weights=ENCODER_WEIGHTS, classes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Loss e Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss combinada do SMP\n",
    "dice_loss = smp.losses.DiceLoss(mode='binary')\n",
    "bce_loss = smp.losses.SoftBCEWithLogitsLoss()\n",
    "\n",
    "def criterion(pred, target):\n",
    "    return 0.5 * bce_loss(pred, target) + 0.5 * dice_loss(pred, target)\n",
    "\n",
    "# Métricas\n",
    "def calc_metrics(pred, target, threshold=0.5):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred_bin = (pred > threshold).float()\n",
    "    \n",
    "    # IoU\n",
    "    intersection = (pred_bin * target).sum()\n",
    "    union = pred_bin.sum() + target.sum() - intersection\n",
    "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "    \n",
    "    # Dice\n",
    "    dice = (2 * intersection + 1e-6) / (pred_bin.sum() + target.sum() + 1e-6)\n",
    "    \n",
    "    return iou.item(), dice.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Funções de Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "    \n",
    "    for images, masks in tqdm(loader, desc='Train'):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        iou, dice = calc_metrics(outputs, masks)\n",
    "        total_iou += iou\n",
    "        total_dice += dice\n",
    "    \n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_iou/n, total_dice/n\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "    \n",
    "    for images, masks in tqdm(loader, desc='Val'):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        iou, dice = calc_metrics(outputs, masks)\n",
    "        total_iou += iou\n",
    "        total_dice += dice\n",
    "    \n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_iou/n, total_dice/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'train_loss': [], 'val_loss': [], 'train_iou': [], 'val_iou': [], \n",
    "           'train_dice': [], 'val_dice': []}\n",
    "best_dice = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'\\nEpoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    \n",
    "    train_loss, train_iou, train_dice = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_iou, val_dice = validate(model, val_loader, criterion)\n",
    "    scheduler.step()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_iou'].append(train_iou)\n",
    "    history['val_iou'].append(val_iou)\n",
    "    history['train_dice'].append(train_dice)\n",
    "    history['val_dice'].append(val_dice)\n",
    "    \n",
    "    print(f'Train - Loss: {train_loss:.4f} | IoU: {train_iou:.4f} | Dice: {train_dice:.4f}')\n",
    "    print(f'Val   - Loss: {val_loss:.4f} | IoU: {val_iou:.4f} | Dice: {val_dice:.4f}')\n",
    "    \n",
    "    if val_dice > best_dice:\n",
    "        best_dice = val_dice\n",
    "        torch.save(model.state_dict(), 'best_optic_disc_model.pth')\n",
    "        print(f'*** Modelo salvo! Dice: {best_dice:.4f} ***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Gráficos de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Treino')\n",
    "axes[0].plot(history['val_loss'], label='Validação')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history['train_iou'], label='Treino')\n",
    "axes[1].plot(history['val_iou'], label='Validação')\n",
    "axes[1].set_title('IoU')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot(history['train_dice'], label='Treino')\n",
    "axes[2].plot(history['val_dice'], label='Validação')\n",
    "axes[2].set_title('Dice Score')\n",
    "axes[2].legend()\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Época')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualizar Predições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar melhor modelo\n",
    "model.load_state_dict(torch.load('best_optic_disc_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "def predict_and_show(dataset, indices):\n",
    "    fig, axes = plt.subplots(len(indices), 4, figsize=(20, 5*len(indices)))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        img, mask = dataset[idx]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = model(img.unsqueeze(0).to(device))\n",
    "            pred = torch.sigmoid(pred).cpu().squeeze().numpy()\n",
    "        \n",
    "        # Desnormalizar imagem\n",
    "        img_np = img.numpy().transpose(1, 2, 0)\n",
    "        img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img_np = np.clip(img_np, 0, 1)\n",
    "        \n",
    "        mask_np = mask.squeeze().numpy()\n",
    "        pred_bin = (pred > 0.5).astype(np.float32)\n",
    "        \n",
    "        # Overlay\n",
    "        overlay = img_np.copy()\n",
    "        overlay[pred_bin > 0.5] = overlay[pred_bin > 0.5] * 0.5 + np.array([0, 1, 0]) * 0.5\n",
    "        \n",
    "        axes[i, 0].imshow(img_np)\n",
    "        axes[i, 0].set_title('Imagem')\n",
    "        axes[i, 1].imshow(mask_np, cmap='gray')\n",
    "        axes[i, 1].set_title('Ground Truth')\n",
    "        axes[i, 2].imshow(pred_bin, cmap='gray')\n",
    "        axes[i, 2].set_title('Predição')\n",
    "        axes[i, 3].imshow(overlay)\n",
    "        axes[i, 3].set_title('Overlay')\n",
    "        \n",
    "        for ax in axes[i]: ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "predict_and_show(val_dataset, [0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Inferência em Nova Imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_image(image_path, model, img_size=512):\n",
    "    \"\"\"Segmenta o disco óptico em uma nova imagem\"\"\"\n",
    "    \n",
    "    # Carregar e preprocessar\n",
    "    image = np.array(Image.open(image_path).convert('RGB'))\n",
    "    original_size = image.shape[:2]\n",
    "    \n",
    "    transform = get_val_transforms()\n",
    "    transformed = transform(image=image)\n",
    "    img_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predição\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(img_tensor)\n",
    "        pred = torch.sigmoid(pred).cpu().squeeze().numpy()\n",
    "    \n",
    "    # Redimensionar máscara para tamanho original\n",
    "    pred_resized = np.array(Image.fromarray((pred * 255).astype(np.uint8)).resize(\n",
    "        (original_size[1], original_size[0]), Image.BILINEAR)) / 255.0\n",
    "    \n",
    "    return pred_resized\n",
    "\n",
    "# Exemplo de uso\n",
    "# mask = segment_image('caminho/para/imagem.jpg', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Avaliação Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular métricas no conjunto de validação\n",
    "model.load_state_dict(torch.load('best_optic_disc_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "all_iou = []\n",
    "all_dice = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in val_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        for i in range(outputs.shape[0]):\n",
    "            iou, dice = calc_metrics(outputs[i:i+1], masks[i:i+1])\n",
    "            all_iou.append(iou)\n",
    "            all_dice.append(dice)\n",
    "\n",
    "print('=== Resultados no Conjunto de Validação ===')\n",
    "print(f'IoU  - Média: {np.mean(all_iou):.4f} | Std: {np.std(all_iou):.4f}')\n",
    "print(f'Dice - Média: {np.mean(all_dice):.4f} | Std: {np.std(all_dice):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EXPERIMENTO 2: Melhorias de Pré-processamento, Data Augmentation e Deep Supervision\n",
    "\n",
    "**Melhorias implementadas:**\n",
    "1. **Pré-processamento avançado**: CLAHE (Contrast Limited Adaptive Histogram Equalization) para realçar estruturas em imagens de fundo de olho\n",
    "2. **Data Augmentation específico para imagens médicas**: ElasticTransform, GridDistortion, OpticalDistortion\n",
    "3. **Deep Supervision**: Múltiplas saídas em diferentes escalas para melhor gradiente\n",
    "4. **Test Time Augmentation (TTA)**: Ensemble de predições com diferentes augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp2.1 - Configurações do Experimento 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXPERIMENTO 2 - Configurações\n",
    "# ============================================\n",
    "\n",
    "# Mesmas configurações base\n",
    "EXP2_BATCH_SIZE = 8\n",
    "EXP2_NUM_EPOCHS = 50\n",
    "EXP2_LEARNING_RATE = 1e-4\n",
    "EXP2_IMG_SIZE = 512\n",
    "\n",
    "EXP2_ENCODER = 'resnet50'\n",
    "EXP2_ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "print(\"=== Experimento 2: Melhorias de Pré-processamento e Data Augmentation ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
