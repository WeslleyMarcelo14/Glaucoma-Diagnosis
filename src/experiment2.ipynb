{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2H-HzBiA8ujX"
   },
   "source": "# EXPERIMENT 2 (Improvement 1): Preprocessing, Data Augmentation and Deep Supervision\n\n**Improvements implemented:**\n1. **CLAHE**: Contrast Limited Adaptive Histogram Equalization to enhance structures\n2. **Medical Data Augmentation**: ElasticTransform, GridDistortion, OpticalDistortion\n3. **Deep Supervision**: Multiple outputs at different scales\n4. **Test Time Augmentation (TTA)**: Ensemble of predictions\n5. **CoarseDropout**: Regularization simulating occlusions"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZhQU3nC8ujd"
   },
   "source": "## 1. Installation and Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xgbj3BQP8uje",
    "outputId": "d30c7626-e3a6-4f7b-eb33-bb084dd7f797"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install segmentation-models-pytorch albumentations opencv-python -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKLXwJ3s8ujh",
    "outputId": "a5b69df9-9e48-4575-9e47-02a34b0ebf06"
   },
   "outputs": [],
   "source": "import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport segmentation_models_pytorch as smp\nfrom skimage.draw import polygon\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {device}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rq5CWKpi8ujh"
   },
   "source": "## 2. Configuration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QRVy2QDh8uji",
    "outputId": "9713fb49-9893-4392-ee60-f3f1b86efe08"
   },
   "outputs": [],
   "source": "# ==== OPTION 1: Mount Google Drive ====\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive', force_remount=True)\n    ROOT_DIR = '/content/drive/MyDrive/PapilaDB/'\n    print(\"Drive mounted!\")\nexcept:\n    print(\"Running locally.\")\n    ROOT_DIR = '/content/PapilaDB/'\n\nprint(f\"ROOT_DIR: {ROOT_DIR}\")\n\n# Hyperparameters\nBATCH_SIZE = 8\nNUM_EPOCHS = 50\nLEARNING_RATE = 1e-4\nIMG_SIZE = 512\n\nENCODER = 'resnet50'\nENCODER_WEIGHTS = 'imagenet'"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiBCAV9g8ujk"
   },
   "source": "## 3. Prepare Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXsGDhQj8ujl",
    "outputId": "7a32f206-bbe4-4176-acc1-b068ddf5764e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Imagens: 488 | Contornos: 976 | Pares: 488\n"
     ]
    }
   ],
   "source": [
    "img_dir = ROOT_DIR + 'FundusImages/'\n",
    "contour_dir = ROOT_DIR + 'ExpertsSegmentations/Contours/'\n",
    "\n",
    "img_files = sorted(os.listdir(img_dir))\n",
    "contour_files = sorted(os.listdir(contour_dir))\n",
    "disc_contours = [f for f in contour_files if 'disc' in f.lower()]\n",
    "\n",
    "def get_pairs():\n",
    "    pairs = []\n",
    "    for img_file in img_files:\n",
    "        img_id = os.path.splitext(img_file)[0]\n",
    "        for cont in disc_contours:\n",
    "            if img_id in cont:\n",
    "                pairs.append({\n",
    "                    'image': os.path.join(img_dir, img_file),\n",
    "                    'contour': os.path.join(contour_dir, cont)\n",
    "                })\n",
    "                break\n",
    "    return pairs\n",
    "\n",
    "pairs = get_pairs()\n",
    "print(f'Imagens: {len(img_files)} | Contornos: {len(disc_contours)} | Pares: {len(pairs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_pswwtL8ujm"
   },
   "source": "## 4. CLAHE Preprocessing"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HTz02fe8ujn"
   },
   "outputs": [],
   "source": "def apply_clahe_preprocessing(image, **kwargs):\n    \"\"\"\n    Applies CLAHE (Contrast Limited Adaptive Histogram Equalization)\n    on the luminance channel to enhance structures in fundus images\n    \"\"\"\n    # Convert to LAB color space\n    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n    l, a, b = cv2.split(lab)\n\n    # Apply CLAHE on L channel (luminance)\n    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n    l_clahe = clahe.apply(l)\n\n    # Recombine channels\n    lab_clahe = cv2.merge([l_clahe, a, b])\n\n    # Convert back to RGB\n    result = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2RGB)\n    return result"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "LYqFY_238ujo",
    "outputId": "0f392cfe-3a0f-4e6d-eadb-a3a449289c73"
   },
   "outputs": [],
   "source": "# Visualize CLAHE effect\ndef compare_clahe_effect(pairs, idx=0):\n    pair = pairs[idx]\n    img_original = np.array(Image.open(pair['image']).convert('RGB'))\n    img_clahe = apply_clahe_preprocessing(img_original)\n\n    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n    axes[0].imshow(img_original)\n    axes[0].set_title('Original Image')\n    axes[0].axis('off')\n    axes[1].imshow(img_clahe)\n    axes[1].set_title('With CLAHE (Enhanced Contrast)')\n    axes[1].axis('off')\n    plt.suptitle('CLAHE Preprocessing Effect', fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\ncompare_clahe_effect(pairs, 0)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxMgbmqW8ujo"
   },
   "source": "## 5. Advanced Data Augmentation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UyH9Fv_78ujp"
   },
   "outputs": [],
   "source": "def get_train_transforms():\n    \"\"\"\n    Advanced Data Augmentation for medical images:\n    - CLAHE as preprocessing\n    - ElasticTransform to simulate anatomical deformations\n    - GridDistortion and OpticalDistortion\n    - CoarseDropout for regularization\n    \"\"\"\n    return A.Compose([\n        # CLAHE preprocessing (always applied)\n        A.Lambda(image=apply_clahe_preprocessing),\n\n        A.Resize(IMG_SIZE, IMG_SIZE),\n\n        # Geometric augmentations\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.RandomRotate90(p=0.5),\n        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=45, p=0.5),\n\n        # Medical image-specific deformations\n        A.OneOf([\n            A.ElasticTransform(alpha=120, sigma=120 * 0.05, p=1.0),\n            A.GridDistortion(num_steps=5, distort_limit=0.3, p=1.0),\n            A.OpticalDistortion(distort_limit=0.5, shift_limit=0.5, p=1.0),\n        ], p=0.4),\n\n        # Color/intensity augmentations\n        A.OneOf([\n            A.GaussNoise(var_limit=(10, 50)),\n            A.GaussianBlur(blur_limit=3),\n            A.MedianBlur(blur_limit=3),\n            A.MotionBlur(blur_limit=3),\n        ], p=0.3),\n\n        A.OneOf([\n            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n            A.CLAHE(clip_limit=4),\n            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20),\n            A.RandomGamma(gamma_limit=(80, 120)),\n        ], p=0.4),\n\n        # Regularization: CoarseDropout (simulates occlusions)\n        A.CoarseDropout(max_holes=8, max_height=32, max_width=32,\n                        min_holes=1, min_height=8, min_width=8,\n                        fill_value=0, p=0.3),\n\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ])\n\ndef get_val_transforms():\n    return A.Compose([\n        A.Lambda(image=apply_clahe_preprocessing),\n        A.Resize(IMG_SIZE, IMG_SIZE),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ])"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4AiMJZY8ujq"
   },
   "source": "## 6. Dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhFHpH4q8ujq"
   },
   "outputs": [],
   "source": [
    "class OpticDiscDataset(Dataset):\n",
    "    def __init__(self, pairs, transforms=None):\n",
    "        self.pairs = pairs\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        image = np.array(Image.open(pair['image']).convert('RGB'))\n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "        contour = np.loadtxt(pair['contour'])\n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        rr, cc = polygon(contour[:, 1], contour[:, 0], mask.shape)\n",
    "        mask[rr, cc] = 1\n",
    "\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "\n",
    "        return image, mask.float().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_JhNECFF8ujr",
    "outputId": "74dcc3fd-17c1-4117-ecbc-d5cca26f4b1f"
   },
   "outputs": [],
   "source": "# Split data\ntrain_pairs, val_pairs = train_test_split(pairs, test_size=0.2, random_state=42)\n\ntrain_dataset = OpticDiscDataset(train_pairs, get_train_transforms())\nval_dataset = OpticDiscDataset(val_pairs, get_val_transforms())\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\nprint(f'Train: {len(train_dataset)} | Validation: {len(val_dataset)}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7I08gu6K8ujs"
   },
   "source": "## 7. Deep Supervision"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lONYw1BY8ujs",
    "outputId": "92e00800-cd50-44f5-fffa-d3e32123b808"
   },
   "outputs": [],
   "source": "class UNetWithDeepSupervision(nn.Module):\n    \"\"\"\n    U-Net with Deep Supervision: uses hooks to capture intermediate features\n    and adds auxiliary outputs at different scales\n    \"\"\"\n    def __init__(self, encoder_name='resnet50', encoder_weights='imagenet',\n                 in_channels=3, classes=1):\n        super().__init__()\n\n        # Base U-Net model\n        self.base_model = smp.Unet(\n            encoder_name=encoder_name,\n            encoder_weights=encoder_weights,\n            in_channels=in_channels,\n            classes=classes,\n            activation=None,\n        )\n\n        # Deep Supervision heads for different scales\n        # These will be applied to encoder features at different levels\n        self.ds_head_1 = nn.Sequential(\n            nn.Conv2d(256, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, classes, kernel_size=1)\n        )\n        self.ds_head_2 = nn.Sequential(\n            nn.Conv2d(512, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, classes, kernel_size=1)\n        )\n\n        self.training_mode = True\n\n    def forward(self, x):\n        # Pass through base model\n        main_output = self.base_model(x)\n\n        if self.training_mode and self.training:\n            # Get encoder features for deep supervision\n            features = self.base_model.encoder(x)\n            # features: [x, stage1, stage2, stage3, stage4, stage5]\n            # For ResNet50: approximate channels [3, 64, 256, 512, 1024, 2048]\n\n            # Deep supervision on intermediate features\n            ds_out_1 = self.ds_head_1(features[2])  # 256 channels, 1/4 resolution\n            ds_out_2 = self.ds_head_2(features[3])  # 512 channels, 1/8 resolution\n\n            # Resize to main output size\n            target_size = main_output.shape[2:]\n            ds_out_1 = nn.functional.interpolate(ds_out_1, size=target_size, mode='bilinear', align_corners=False)\n            ds_out_2 = nn.functional.interpolate(ds_out_2, size=target_size, mode='bilinear', align_corners=False)\n\n            return main_output, ds_out_1, ds_out_2\n\n        return main_output\n\ndef deep_supervision_loss(outputs, target, criterion, weights=[1.0, 0.4, 0.2]):\n    \"\"\"Calculates combined loss for deep supervision\"\"\"\n    if isinstance(outputs, tuple):\n        main_out, ds1, ds2 = outputs\n        loss = weights[0] * criterion(main_out, target)\n        loss += weights[1] * criterion(ds1, target)\n        loss += weights[2] * criterion(ds2, target)\n        return loss, main_out\n    else:\n        return criterion(outputs, target), outputs\n\nprint(\"Deep Supervision defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPzzK84f8ujt"
   },
   "source": "## 8. Test Time Augmentation (TTA)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJyFKlXA8ujt"
   },
   "outputs": [],
   "source": "class TestTimeAugmentation:\n    \"\"\"\n    Test Time Augmentation: makes multiple predictions with different\n    augmentations and combines results for more robust prediction\n    \"\"\"\n    def __init__(self, model, device):\n        self.model = model\n        self.device = device\n\n    def __call__(self, image):\n        self.model.eval()\n        predictions = []\n\n        with torch.no_grad():\n            # Original\n            pred = torch.sigmoid(self.model(image))\n            predictions.append(pred)\n\n            # Horizontal flip\n            flipped_h = torch.flip(image, dims=[3])\n            pred_h = torch.sigmoid(self.model(flipped_h))\n            pred_h = torch.flip(pred_h, dims=[3])\n            predictions.append(pred_h)\n\n            # Vertical flip\n            flipped_v = torch.flip(image, dims=[2])\n            pred_v = torch.sigmoid(self.model(flipped_v))\n            pred_v = torch.flip(pred_v, dims=[2])\n            predictions.append(pred_v)\n\n            # Both flips\n            flipped_hv = torch.flip(image, dims=[2, 3])\n            pred_hv = torch.sigmoid(self.model(flipped_hv))\n            pred_hv = torch.flip(pred_hv, dims=[2, 3])\n            predictions.append(pred_hv)\n\n            # Rotations\n            for k in [1, 2, 3]:\n                rotated = torch.rot90(image, k=k, dims=[2, 3])\n                pred_rot = torch.sigmoid(self.model(rotated))\n                pred_rot = torch.rot90(pred_rot, k=-k, dims=[2, 3])\n                predictions.append(pred_rot)\n\n        return torch.stack(predictions).mean(dim=0)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bSF7Y6i8uju"
   },
   "source": "## 9. Model, Loss and Optimizer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240,
     "referenced_widgets": [
      "56fc55afdbb64d3eb7e6e57eab18edc6",
      "c392159ba7664cedb09f8344145cb242",
      "46670d02e66f4301baa4f16637e7c409",
      "7dc48834bbc84e27b3509784a707e94e",
      "2f9ecf6c65f8450697b659832b35126d",
      "c01de03a8a1243d2a75bdea4df9655e5",
      "e9c86789068848cc88f04785214a064c",
      "76712b9b94c2457b96d2e0682af50a8b",
      "fa2743241dd74a24b5137f9526a7f7a7",
      "4e5ecc97e2714fbc979e95e9a7a4cb56",
      "8ab742f564ab47a68458cc89e07aa4d3",
      "4d86fc5ac11747958c9ca325581b0527",
      "755d893240b5443aa47d5985d122fbf2",
      "608d4f28c4774b3381f5131c8d24b7c6",
      "4a92eb8726044f948360d0abeb297f32",
      "8ac8e4cda79347d1a05633e37bc4ba66",
      "8b47f23a3adf45009bc8c57c534143e1",
      "a5bf64e765084aee81acd86850be9034",
      "c2555be3ebee4f809eb09fbec1c2a273",
      "7a57a4fe59064c6d941d34b6be546994",
      "c1fb035ec3de4282886664550f2e27bd",
      "a2f07e3472f94c5cb3f31a1f70222534"
     ]
    },
    "id": "iolqukJP8uju",
    "outputId": "4a3e555f-849e-4e33-b6fd-830ec37e85fb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56fc55afdbb64d3eb7e6e57eab18edc6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d86fc5ac11747958c9ca325581b0527"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Modelo: U-Net com Deep Supervision\n",
      "Encoder: resnet50\n"
     ]
    }
   ],
   "source": [
    "# Criar modelo\n",
    "model = UNetWithDeepSupervision(\n",
    "    encoder_name=ENCODER,\n",
    "    encoder_weights=ENCODER_WEIGHTS,\n",
    "    in_channels=3,\n",
    "    classes=1\n",
    ").to(device)\n",
    "\n",
    "# Loss\n",
    "dice_loss = smp.losses.DiceLoss(mode='binary')\n",
    "bce_loss = smp.losses.SoftBCEWithLogitsLoss()\n",
    "\n",
    "def criterion(pred, target):\n",
    "    return 0.5 * bce_loss(pred, target) + 0.5 * dice_loss(pred, target)\n",
    "\n",
    "# Métricas\n",
    "def calc_metrics(pred, target, threshold=0.5):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred_bin = (pred > threshold).float()\n",
    "    intersection = (pred_bin * target).sum()\n",
    "    union = pred_bin.sum() + target.sum() - intersection\n",
    "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "    dice = (2 * intersection + 1e-6) / (pred_bin.sum() + target.sum() + 1e-6)\n",
    "    return iou.item(), dice.item()\n",
    "\n",
    "# Otimizador com scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "print(f'Modelo: U-Net com Deep Supervision')\n",
    "print(f'Encoder: {ENCODER}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFYF3Njq8uju"
   },
   "source": "## 10. Training Functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HodY3-Fs8ujv"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "\n",
    "    for images, masks in tqdm(loader, desc='Train'):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss, main_output = deep_supervision_loss(outputs, masks, criterion)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        iou, dice = calc_metrics(main_output, masks)\n",
    "        total_iou += iou\n",
    "        total_dice += dice\n",
    "\n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_iou/n, total_dice/n\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "\n",
    "    for images, masks in tqdm(loader, desc='Val'):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        iou, dice = calc_metrics(outputs, masks)\n",
    "        total_iou += iou\n",
    "        total_dice += dice\n",
    "\n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_iou/n, total_dice/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VNji4438ujv"
   },
   "source": "## 11. Training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HlqF5c-88ujw",
    "outputId": "ca0215c4-c1f3-4ba0-fea7-5cbdf879d659"
   },
   "outputs": [],
   "source": "history = {'train_loss': [], 'val_loss': [], 'train_iou': [], 'val_iou': [],\n           'train_dice': [], 'val_dice': []}\nbest_dice = 0\n\nprint(\"=\"*60)\nprint(\"EXPERIMENT 2 - CLAHE + Data Aug + Deep Supervision\")\nprint(\"=\"*60)\n\nfor epoch in range(NUM_EPOCHS):\n    print(f'\\nEpoch {epoch+1}/{NUM_EPOCHS}')\n\n    train_loss, train_iou, train_dice = train_epoch(model, train_loader, criterion, optimizer)\n    val_loss, val_iou, val_dice = validate(model, val_loader, criterion)\n    scheduler.step()\n\n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    history['train_iou'].append(train_iou)\n    history['val_iou'].append(val_iou)\n    history['train_dice'].append(train_dice)\n    history['val_dice'].append(val_dice)\n\n    print(f'Train - Loss: {train_loss:.4f} | IoU: {train_iou:.4f} | Dice: {train_dice:.4f}')\n    print(f'Val   - Loss: {val_loss:.4f} | IoU: {val_iou:.4f} | Dice: {val_dice:.4f}')\n\n    if val_dice > best_dice:\n        best_dice = val_dice\n        torch.save(model.state_dict(), 'best_exp2_model.pth')\n        print(f'*** Model saved! Dice: {best_dice:.4f} ***')\n\nprint(f\"\\nBest Dice: {best_dice:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1t1Vd4b48ujw"
   },
   "source": "## 12. Training Graphs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2VbVKZO8ujx",
    "outputId": "69bc37d4-7ce2-48f9-b0ad-eecd4ec6bc58"
   },
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\naxes[0].plot(history['train_loss'], label='Train')\naxes[0].plot(history['val_loss'], label='Validation')\naxes[0].set_title('Loss')\naxes[0].legend()\n\naxes[1].plot(history['train_iou'], label='Train')\naxes[1].plot(history['val_iou'], label='Validation')\naxes[1].set_title('IoU')\naxes[1].legend()\n\naxes[2].plot(history['train_dice'], label='Train')\naxes[2].plot(history['val_dice'], label='Validation')\naxes[2].set_title('Dice Score')\naxes[2].legend()\n\nfor ax in axes:\n    ax.set_xlabel('Epoch')\n    ax.grid(True, alpha=0.3)\n\nplt.suptitle('Experiment 2: CLAHE + Data Aug + Deep Supervision', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcr78Zsi8ujx"
   },
   "source": "## 13. Evaluation with TTA"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v_Kpr_6A8ujy",
    "outputId": "e6bf2dea-7a0a-4c9a-dffd-7e6fc42f6563"
   },
   "outputs": [],
   "source": "# Load best model\nmodel.load_state_dict(torch.load('best_exp2_model.pth'))\nmodel.eval()\n\ntta = TestTimeAugmentation(model, device)\n\n# Evaluation\nall_iou_no_tta = []\nall_dice_no_tta = []\nall_iou_tta = []\nall_dice_tta = []\n\nprint(\"Evaluating (with and without TTA)...\")\n\nwith torch.no_grad():\n    for images, masks in tqdm(val_loader):\n        images, masks = images.to(device), masks.to(device)\n\n        for i in range(images.shape[0]):\n            img = images[i:i+1]\n            mask = masks[i:i+1]\n\n            # Without TTA\n            pred = torch.sigmoid(model(img))\n            pred_bin = (pred > 0.5).float()\n            intersection = (pred_bin * mask).sum()\n            union = pred_bin.sum() + mask.sum() - intersection\n            all_iou_no_tta.append(((intersection + 1e-6) / (union + 1e-6)).item())\n            all_dice_no_tta.append(((2 * intersection + 1e-6) / (pred_bin.sum() + mask.sum() + 1e-6)).item())\n\n            # With TTA\n            pred_tta = tta(img)\n            pred_bin_tta = (pred_tta > 0.5).float()\n            intersection = (pred_bin_tta * mask).sum()\n            union = pred_bin_tta.sum() + mask.sum() - intersection\n            all_iou_tta.append(((intersection + 1e-6) / (union + 1e-6)).item())\n            all_dice_tta.append(((2 * intersection + 1e-6) / (pred_bin_tta.sum() + mask.sum() + 1e-6)).item())\n\nprint('\\n' + '='*60)\nprint('RESULTS - EXPERIMENT 2')\nprint('='*60)\nprint('\\nWithout TTA:')\nprint(f'  IoU:  {np.mean(all_iou_no_tta):.4f} +/- {np.std(all_iou_no_tta):.4f}')\nprint(f'  Dice: {np.mean(all_dice_no_tta):.4f} +/- {np.std(all_dice_no_tta):.4f}')\nprint('\\nWith TTA (7 augmentations):')\nprint(f'  IoU:  {np.mean(all_iou_tta):.4f} +/- {np.std(all_iou_tta):.4f}')\nprint(f'  Dice: {np.mean(all_dice_tta):.4f} +/- {np.std(all_dice_tta):.4f}')\nprint(f'\\nImprovement with TTA: +{(np.mean(all_dice_tta) - np.mean(all_dice_no_tta))*100:.2f}% Dice')"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV5othCB8ujz"
   },
   "source": "## 14. Visualize Predictions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HN4NstHe8ujz",
    "outputId": "802f8390-274f-450f-f343-b805b0441fde"
   },
   "outputs": [],
   "source": "def predict_and_show(dataset, indices):\n    fig, axes = plt.subplots(len(indices), 4, figsize=(20, 5*len(indices)))\n\n    for i, idx in enumerate(indices):\n        img, mask = dataset[idx]\n\n        with torch.no_grad():\n            pred = model(img.unsqueeze(0).to(device))\n            pred = torch.sigmoid(pred).cpu().squeeze().numpy()\n\n        img_np = img.numpy().transpose(1, 2, 0)\n        img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n        img_np = np.clip(img_np, 0, 1)\n\n        mask_np = mask.squeeze().numpy()\n        pred_bin = (pred > 0.5).astype(np.float32)\n\n        overlay = img_np.copy()\n        overlay[pred_bin > 0.5] = overlay[pred_bin > 0.5] * 0.5 + np.array([0, 1, 0]) * 0.5\n\n        axes[i, 0].imshow(img_np)\n        axes[i, 0].set_title('Image')\n        axes[i, 1].imshow(mask_np, cmap='gray')\n        axes[i, 1].set_title('Ground Truth')\n        axes[i, 2].imshow(pred_bin, cmap='gray')\n        axes[i, 2].set_title('Prediction')\n        axes[i, 3].imshow(overlay)\n        axes[i, 3].set_title('Overlay')\n\n        for ax in axes[i]: ax.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\npredict_and_show(val_dataset, [0, 1, 2, 3])"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IESveigb8uj0"
   },
   "source": "## 15. Save Results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IsbO7eiA8uj4",
    "outputId": "07eead91-f7c4-41a3-9998-43cc709863d2"
   },
   "outputs": [],
   "source": "import pickle\n\nresults_exp2 = {\n    'history': history,\n    'all_iou_no_tta': all_iou_no_tta,\n    'all_dice_no_tta': all_dice_no_tta,\n    'all_iou_tta': all_iou_tta,\n    'all_dice_tta': all_dice_tta,\n    'best_dice': best_dice\n}\n\nwith open('results_exp2.pkl', 'wb') as f:\n    pickle.dump(results_exp2, f)\n\nprint('Results saved to results_exp2.pkl')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "56fc55afdbb64d3eb7e6e57eab18edc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c392159ba7664cedb09f8344145cb242",
       "IPY_MODEL_46670d02e66f4301baa4f16637e7c409",
       "IPY_MODEL_7dc48834bbc84e27b3509784a707e94e"
      ],
      "layout": "IPY_MODEL_2f9ecf6c65f8450697b659832b35126d"
     }
    },
    "c392159ba7664cedb09f8344145cb242": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c01de03a8a1243d2a75bdea4df9655e5",
      "placeholder": "​",
      "style": "IPY_MODEL_e9c86789068848cc88f04785214a064c",
      "value": "config.json: 100%"
     }
    },
    "46670d02e66f4301baa4f16637e7c409": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76712b9b94c2457b96d2e0682af50a8b",
      "max": 156,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fa2743241dd74a24b5137f9526a7f7a7",
      "value": 156
     }
    },
    "7dc48834bbc84e27b3509784a707e94e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e5ecc97e2714fbc979e95e9a7a4cb56",
      "placeholder": "​",
      "style": "IPY_MODEL_8ab742f564ab47a68458cc89e07aa4d3",
      "value": " 156/156 [00:00&lt;00:00, 17.1kB/s]"
     }
    },
    "2f9ecf6c65f8450697b659832b35126d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c01de03a8a1243d2a75bdea4df9655e5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9c86789068848cc88f04785214a064c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "76712b9b94c2457b96d2e0682af50a8b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa2743241dd74a24b5137f9526a7f7a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4e5ecc97e2714fbc979e95e9a7a4cb56": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ab742f564ab47a68458cc89e07aa4d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4d86fc5ac11747958c9ca325581b0527": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_755d893240b5443aa47d5985d122fbf2",
       "IPY_MODEL_608d4f28c4774b3381f5131c8d24b7c6",
       "IPY_MODEL_4a92eb8726044f948360d0abeb297f32"
      ],
      "layout": "IPY_MODEL_8ac8e4cda79347d1a05633e37bc4ba66"
     }
    },
    "755d893240b5443aa47d5985d122fbf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b47f23a3adf45009bc8c57c534143e1",
      "placeholder": "​",
      "style": "IPY_MODEL_a5bf64e765084aee81acd86850be9034",
      "value": "model.safetensors: 100%"
     }
    },
    "608d4f28c4774b3381f5131c8d24b7c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2555be3ebee4f809eb09fbec1c2a273",
      "max": 102464800,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7a57a4fe59064c6d941d34b6be546994",
      "value": 102464800
     }
    },
    "4a92eb8726044f948360d0abeb297f32": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1fb035ec3de4282886664550f2e27bd",
      "placeholder": "​",
      "style": "IPY_MODEL_a2f07e3472f94c5cb3f31a1f70222534",
      "value": " 102M/102M [00:01&lt;00:00, 47.0MB/s]"
     }
    },
    "8ac8e4cda79347d1a05633e37bc4ba66": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b47f23a3adf45009bc8c57c534143e1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5bf64e765084aee81acd86850be9034": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c2555be3ebee4f809eb09fbec1c2a273": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a57a4fe59064c6d941d34b6be546994": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c1fb035ec3de4282886664550f2e27bd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2f07e3472f94c5cb3f31a1f70222534": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}