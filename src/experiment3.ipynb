{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQqfeiPfv2p1"
   },
   "source": "# EXPERIMENT 3 (Improvement 2): Attention U-Net - Architecture Modification\n\n**Neural network architecture modifications:**\n1. **Attention Gates**: Attention mechanisms in skip connections\n2. **Squeeze-and-Excitation (SE) Blocks**: Adaptive channel recalibration\n3. **ASPP (Atrous Spatial Pyramid Pooling)**: Multi-scale context at bottleneck\n4. **Focal Loss**: To handle class imbalance\n\nThis is a substantial modification to the network topology, not just an increase in width/depth."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpMgBX_Sv2p9"
   },
   "source": "## 1. Installation and Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBFdnOTnv2p_"
   },
   "outputs": [],
   "source": [
    "!pip install segmentation-models-pytorch albumentations opencv-python -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aO5dcWXIv2qB",
    "outputId": "b67c6f98-e3c6-4a27-d805-6d1e30035cbe"
   },
   "outputs": [],
   "source": "import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport segmentation_models_pytorch as smp\nfrom skimage.draw import polygon\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {device}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aeD1kJEv2qC"
   },
   "source": "## 2. Configuration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FOS1XqLv2qD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1158f37b-301f-4f5e-8b6f-d7352a3b8a05"
   },
   "outputs": [],
   "source": "try:\n    from google.colab import drive\n    drive.mount('/content/drive', force_remount=True)\n    ROOT_DIR = '/content/drive/MyDrive/PapilaDB/'\n    print(\"Drive mounted!\")\nexcept:\n    print(\"Running locally.\")\n    ROOT_DIR = '/content/PapilaDB/'\n\n# Hyperparameters\nBATCH_SIZE = 8\nNUM_EPOCHS = 50\nLEARNING_RATE = 1e-4\nIMG_SIZE = 512\n\nENCODER = 'resnet50'\nENCODER_WEIGHTS = 'imagenet'"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAxl1DpMv2qE"
   },
   "source": "## 3. Prepare Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWcCoiLYv2qF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bb99afae-10ea-4445-b073-4eda0d9b8ba2"
   },
   "outputs": [],
   "source": "img_dir = ROOT_DIR + 'FundusImages/'\ncontour_dir = ROOT_DIR + 'ExpertsSegmentations/Contours/'\n\nimg_files = sorted(os.listdir(img_dir))\ncontour_files = sorted(os.listdir(contour_dir))\ndisc_contours = [f for f in contour_files if 'disc' in f.lower()]\n\ndef get_pairs():\n    pairs = []\n    for img_file in img_files:\n        img_id = os.path.splitext(img_file)[0]\n        for cont in disc_contours:\n            if img_id in cont:\n                pairs.append({\n                    'image': os.path.join(img_dir, img_file),\n                    'contour': os.path.join(contour_dir, cont)\n                })\n                break\n    return pairs\n\npairs = get_pairs()\nprint(f'Images: {len(img_files)} | Contours: {len(disc_contours)} | Pairs: {len(pairs)}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlBBvvD1v2qG"
   },
   "source": "## 4. Preprocessing and Data Augmentation\n\nUsing the same transforms from Exp2 for fair comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4XhfWHmv2qH"
   },
   "outputs": [],
   "source": [
    "def apply_clahe_preprocessing(image, **kwargs):\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    l_clahe = clahe.apply(l)\n",
    "    lab_clahe = cv2.merge([l_clahe, a, b])\n",
    "    return cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "def get_train_transforms():\n",
    "    return A.Compose([\n",
    "        A.Lambda(image=apply_clahe_preprocessing),\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=45, p=0.5),\n",
    "        A.OneOf([\n",
    "            A.ElasticTransform(alpha=120, sigma=120 * 0.05, p=1.0),\n",
    "            A.GridDistortion(num_steps=5, distort_limit=0.3, p=1.0),\n",
    "            A.OpticalDistortion(distort_limit=0.5, shift_limit=0.5, p=1.0),\n",
    "        ], p=0.4),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(var_limit=(10, 50)),\n",
    "            A.GaussianBlur(blur_limit=3),\n",
    "            A.MedianBlur(blur_limit=3),\n",
    "        ], p=0.3),\n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n",
    "            A.CLAHE(clip_limit=4),\n",
    "            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20),\n",
    "        ], p=0.4),\n",
    "        A.CoarseDropout(max_holes=8, max_height=32, max_width=32,\n",
    "                        min_holes=1, min_height=8, min_width=8, fill_value=0, p=0.3),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_val_transforms():\n",
    "    return A.Compose([\n",
    "        A.Lambda(image=apply_clahe_preprocessing),\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4weqZg7v2qJ"
   },
   "source": "## 5. Dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEtb0RVlv2qJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ef051f1a-9e1d-4c4d-c120-767a8a867c8e"
   },
   "outputs": [],
   "source": "class OpticDiscDataset(Dataset):\n    def __init__(self, pairs, transforms=None):\n        self.pairs = pairs\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        pair = self.pairs[idx]\n        image = np.array(Image.open(pair['image']).convert('RGB'))\n        h, w = image.shape[:2]\n\n        contour = np.loadtxt(pair['contour'])\n        mask = np.zeros((h, w), dtype=np.uint8)\n        rr, cc = polygon(contour[:, 1], contour[:, 0], mask.shape)\n        mask[rr, cc] = 1\n\n        if self.transforms:\n            transformed = self.transforms(image=image, mask=mask)\n            image = transformed['image']\n            mask = transformed['mask']\n\n        return image, mask.float().unsqueeze(0)\n\n# Split\ntrain_pairs, val_pairs = train_test_split(pairs, test_size=0.2, random_state=42)\n\ntrain_dataset = OpticDiscDataset(train_pairs, get_train_transforms())\nval_dataset = OpticDiscDataset(val_pairs, get_val_transforms())\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\nprint(f'Train: {len(train_dataset)} | Validation: {len(val_dataset)}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofcM7iVWv2qK"
   },
   "source": "## 6. Attention Modules"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00KGDK2Qv2qK"
   },
   "outputs": [],
   "source": "class AttentionGate(nn.Module):\n    \"\"\"\n    Attention Gate: Allows the model to focus on relevant regions\n    in skip connections, suppressing irrelevant responses.\n\n    Ref: \"Attention U-Net: Learning Where to Look for the Pancreas\"\n    \"\"\"\n    def __init__(self, F_g, F_l, F_int):\n        super().__init__()\n\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, g, x):\n        if g.shape[2:] != x.shape[2:]:\n            g = nn.functional.interpolate(g, size=x.shape[2:], mode='bilinear', align_corners=False)\n\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n\n        return x * psi"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t87UqKddv2qL"
   },
   "outputs": [],
   "source": "class SqueezeExcitation(nn.Module):\n    \"\"\"\n    Squeeze-and-Excitation Block: Adaptive channel recalibration.\n    Learns the relative importance of each feature channel.\n\n    Ref: \"Squeeze-and-Excitation Networks\"\n    \"\"\"\n    def __init__(self, channels, reduction=16):\n        super().__init__()\n\n        self.squeeze = nn.AdaptiveAvgPool2d(1)\n        self.excitation = nn.Sequential(\n            nn.Linear(channels, channels // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channels // reduction, channels, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.shape\n        y = self.squeeze(x).view(b, c)\n        y = self.excitation(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VX2qBkcAv2qL"
   },
   "outputs": [],
   "source": "class ASPP(nn.Module):\n    \"\"\"\n    Atrous Spatial Pyramid Pooling: Captures multi-scale context\n    using dilated convolutions with different rates.\n\n    Ref: \"DeepLab: Semantic Image Segmentation\"\n    \"\"\"\n    def __init__(self, in_channels, out_channels, rates=[6, 12, 18]):\n        super().__init__()\n\n        self.conv1x1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n        self.atrous_convs = nn.ModuleList()\n        for rate in rates:\n            self.atrous_convs.append(\n                nn.Sequential(\n                    nn.Conv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False),\n                    nn.BatchNorm2d(out_channels),\n                    nn.ReLU(inplace=True)\n                )\n            )\n\n        self.global_pool = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n        num_features = out_channels * (2 + len(rates))\n        self.project = nn.Sequential(\n            nn.Conv2d(num_features, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5)\n        )\n\n    def forward(self, x):\n        size = x.shape[2:]\n        features = [self.conv1x1(x)]\n\n        for atrous_conv in self.atrous_convs:\n            features.append(atrous_conv(x))\n\n        global_feat = self.global_pool(x)\n        global_feat = nn.functional.interpolate(global_feat, size=size, mode='bilinear', align_corners=False)\n        features.append(global_feat)\n\n        x = torch.cat(features, dim=1)\n        return self.project(x)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFTcyjNav2qM"
   },
   "source": "## 7. Complete Attention U-Net"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StLtanlGv2qN"
   },
   "outputs": [],
   "source": "class AttentionUNet(nn.Module):\n    \"\"\"\n    Attention U-Net with:\n    - Pretrained ResNet50 encoder\n    - Attention Gates in skip connections\n    - SE blocks in decoder\n    - ASPP at bottleneck\n    \"\"\"\n    def __init__(self, encoder_name='resnet50', encoder_weights='imagenet',\n                 in_channels=3, classes=1):\n        super().__init__()\n\n        self.encoder = smp.encoders.get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=5,\n            weights=encoder_weights\n        )\n\n        # Get encoder channels dynamically\n        encoder_channels = self.encoder.out_channels\n        print(f\"Encoder channels: {encoder_channels}\")\n\n        # For ResNet50: [3, 64, 256, 512, 1024, 2048]\n        # skips will be: [1024, 512, 256, 64] (features[4] to features[1], reversed)\n\n        # ASPP at bottleneck\n        self.bottleneck_ch = encoder_channels[-1]  # 2048\n        self.aspp_out = self.bottleneck_ch // 2     # 1024\n        self.aspp = ASPP(self.bottleneck_ch, self.aspp_out)\n\n        # Skip connection channels (reversed encoder features, excluding bottleneck and input)\n        # features[:-1] = [3, 64, 256, 512, 1024], [::-1] = [1024, 512, 256, 64, 3]\n        # We only use the first 4 for skip connections\n        skip_channels = list(encoder_channels[:-1])[::-1][:4]  # [1024, 512, 256, 64]\n        print(f\"Skip channels: {skip_channels}\")\n\n        decoder_channels = [256, 128, 64, 32, 16]\n\n        # Attention Gates\n        # Gate 0: g comes from ASPP (aspp_out), x comes from skip[0]\n        # Gates 1-3: g comes from previous decoder, x comes from corresponding skip\n        self.attention_gates = nn.ModuleList()\n\n        # First attention gate: g = ASPP output\n        self.attention_gates.append(\n            AttentionGate(self.aspp_out, skip_channels[0], skip_channels[0] // 4)\n        )\n\n        # Remaining attention gates: g = output from previous decoder\n        for i in range(1, 4):\n            self.attention_gates.append(\n                AttentionGate(decoder_channels[i-1], skip_channels[i], skip_channels[i] // 4)\n            )\n\n        # Decoder blocks\n        self.decoder_blocks = nn.ModuleList()\n\n        # First block: ASPP output + skip[0]\n        in_ch = self.aspp_out + skip_channels[0]\n        self.decoder_blocks.append(self._make_decoder_block(in_ch, decoder_channels[0]))\n\n        # Following blocks: decoder[i-1] + skip[i]\n        for i in range(1, 4):\n            in_ch = decoder_channels[i-1] + skip_channels[i]\n            self.decoder_blocks.append(self._make_decoder_block(in_ch, decoder_channels[i]))\n\n        # Last block (no skip connection)\n        self.decoder_blocks.append(self._make_decoder_block(decoder_channels[3], decoder_channels[4]))\n\n        self.segmentation_head = nn.Conv2d(decoder_channels[-1], classes, kernel_size=1)\n\n        # Store for debug\n        self.skip_channels = skip_channels\n        self.decoder_channels = decoder_channels\n\n    def _make_decoder_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            SqueezeExcitation(out_channels, reduction=16)\n        )\n\n    def forward(self, x):\n        # Encoder\n        features = self.encoder(x)\n\n        # Bottleneck with ASPP\n        x = self.aspp(features[-1])\n\n        # Skip connections (reversed, excluding bottleneck and input)\n        skips = features[1:-1][::-1]  # [features[4], features[3], features[2], features[1]]\n\n        # Decoder with attention gates\n        for i in range(4):\n            x = nn.functional.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n            skip = self.attention_gates[i](x, skips[i])\n            x = torch.cat([x, skip], dim=1)\n            x = self.decoder_blocks[i](x)\n\n        # Last upsample (no skip)\n        x = nn.functional.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n        x = self.decoder_blocks[4](x)\n\n        return self.segmentation_head(x)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1q9Dlb3v2qN"
   },
   "source": "## 8. Test Time Augmentation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLgAEOlGv2qN"
   },
   "outputs": [],
   "source": [
    "class TestTimeAugmentation:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, image):\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = torch.sigmoid(self.model(image))\n",
    "            predictions.append(pred)\n",
    "\n",
    "            for flip_dims in [[3], [2], [2, 3]]:\n",
    "                flipped = torch.flip(image, dims=flip_dims)\n",
    "                pred_f = torch.sigmoid(self.model(flipped))\n",
    "                pred_f = torch.flip(pred_f, dims=flip_dims)\n",
    "                predictions.append(pred_f)\n",
    "\n",
    "            for k in [1, 2, 3]:\n",
    "                rotated = torch.rot90(image, k=k, dims=[2, 3])\n",
    "                pred_rot = torch.sigmoid(self.model(rotated))\n",
    "                pred_rot = torch.rot90(pred_rot, k=-k, dims=[2, 3])\n",
    "                predictions.append(pred_rot)\n",
    "\n",
    "        return torch.stack(predictions).mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsunmOetv2qN"
   },
   "source": "## 9. Model, Loss and Optimizer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-s6dRqWuv2qO",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fe0cc278-6de2-4f97-8915-efdee6d2cdbb"
   },
   "outputs": [],
   "source": "# Create model\nmodel = AttentionUNet(\n    encoder_name=ENCODER,\n    encoder_weights=ENCODER_WEIGHTS,\n    in_channels=3,\n    classes=1\n).to(device)\n\n# Count parameters\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'Model: Attention U-Net')\nprint(f'Trainable parameters: {count_parameters(model):,}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SI95J2Vtv2qO"
   },
   "outputs": [],
   "source": "# Combined loss with Focal Loss\ndice_loss = smp.losses.DiceLoss(mode='binary')\nbce_loss = smp.losses.SoftBCEWithLogitsLoss()\nfocal_loss = smp.losses.FocalLoss(mode='binary', alpha=0.25, gamma=2.0)\n\ndef criterion(pred, target):\n    return 0.4 * bce_loss(pred, target) + 0.4 * dice_loss(pred, target) + 0.2 * focal_loss(pred, target)\n\n# Metrics\ndef calc_metrics(pred, target, threshold=0.5):\n    pred = torch.sigmoid(pred)\n    pred_bin = (pred > threshold).float()\n    intersection = (pred_bin * target).sum()\n    union = pred_bin.sum() + target.sum() - intersection\n    iou = (intersection + 1e-6) / (union + 1e-6)\n    dice = (2 * intersection + 1e-6) / (pred_bin.sum() + target.sum() + 1e-6)\n    return iou.item(), dice.item()\n\n# Optimizer with differentiated LR\nencoder_params = list(model.encoder.parameters())\ndecoder_params = [p for n, p in model.named_parameters() if 'encoder' not in n]\n\noptimizer = optim.AdamW([\n    {'params': encoder_params, 'lr': LEARNING_RATE * 0.1},\n    {'params': decoder_params, 'lr': LEARNING_RATE}\n], weight_decay=1e-4)\n\nscheduler = optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=[LEARNING_RATE * 0.1, LEARNING_RATE],\n    epochs=NUM_EPOCHS,\n    steps_per_epoch=len(train_loader),\n    pct_start=0.1,\n    anneal_strategy='cos'\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02mEAV1iv2qO"
   },
   "source": "## 10. Training Functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TGItQUQ8v2qO"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "\n",
    "    for images, masks in tqdm(loader, desc='Train'):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        iou, dice = calc_metrics(outputs, masks)\n",
    "        total_iou += iou\n",
    "        total_dice += dice\n",
    "\n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_iou/n, total_dice/n\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "\n",
    "    for images, masks in tqdm(loader, desc='Val'):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        iou, dice = calc_metrics(outputs, masks)\n",
    "        total_iou += iou\n",
    "        total_dice += dice\n",
    "\n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_iou/n, total_dice/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpcFalGRv2qO"
   },
   "source": "## 11. Training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njHvtgD8v2qO",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cb9ef02e-4837-40b4-a66c-a109cc4cafbc"
   },
   "outputs": [],
   "source": "history = {'train_loss': [], 'val_loss': [], 'train_iou': [], 'val_iou': [],\n           'train_dice': [], 'val_dice': []}\nbest_dice = 0\n\nprint(\"=\"*60)\nprint(\"EXPERIMENT 3 - Attention U-Net\")\nprint(\"=\"*60)\n\nfor epoch in range(NUM_EPOCHS):\n    print(f'\\nEpoch {epoch+1}/{NUM_EPOCHS}')\n\n    train_loss, train_iou, train_dice = train_epoch(model, train_loader, criterion, optimizer, scheduler)\n    val_loss, val_iou, val_dice = validate(model, val_loader, criterion)\n\n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    history['train_iou'].append(train_iou)\n    history['val_iou'].append(val_iou)\n    history['train_dice'].append(train_dice)\n    history['val_dice'].append(val_dice)\n\n    current_lr = optimizer.param_groups[1]['lr']\n    print(f'Train - Loss: {train_loss:.4f} | IoU: {train_iou:.4f} | Dice: {train_dice:.4f}')\n    print(f'Val   - Loss: {val_loss:.4f} | IoU: {val_iou:.4f} | Dice: {val_dice:.4f}')\n    print(f'LR: {current_lr:.2e}')\n\n    if val_dice > best_dice:\n        best_dice = val_dice\n        torch.save(model.state_dict(), 'best_exp3_attention_unet.pth')\n        print(f'*** Model saved! Dice: {best_dice:.4f} ***')\n\nprint(f\"\\nBest Dice: {best_dice:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lz6Cfu1Lv2qP"
   },
   "source": "## 12. Training Graphs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkFrtlYuv2qP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3a147247-b633-410e-c05e-967387c01498"
   },
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\naxes[0].plot(history['train_loss'], label='Train')\naxes[0].plot(history['val_loss'], label='Validation')\naxes[0].set_title('Loss')\naxes[0].legend()\n\naxes[1].plot(history['train_iou'], label='Train')\naxes[1].plot(history['val_iou'], label='Validation')\naxes[1].set_title('IoU')\naxes[1].legend()\n\naxes[2].plot(history['train_dice'], label='Train')\naxes[2].plot(history['val_dice'], label='Validation')\naxes[2].set_title('Dice Score')\naxes[2].legend()\n\nfor ax in axes:\n    ax.set_xlabel('Epoch')\n    ax.grid(True, alpha=0.3)\n\nplt.suptitle('Experiment 3: Attention U-Net', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYVRbP0Ev2qP"
   },
   "source": "## 13. Evaluation with TTA"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqL0qv8Iv2qP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "462a561f-ad67-45c5-c6bc-a051ab072b0e"
   },
   "outputs": [],
   "source": "model.load_state_dict(torch.load('best_exp3_attention_unet.pth'))\nmodel.eval()\n\ntta = TestTimeAugmentation(model, device)\n\nall_iou_no_tta = []\nall_dice_no_tta = []\nall_iou_tta = []\nall_dice_tta = []\n\nprint(\"Evaluating (with and without TTA)...\")\n\nwith torch.no_grad():\n    for images, masks in tqdm(val_loader):\n        images, masks = images.to(device), masks.to(device)\n\n        for i in range(images.shape[0]):\n            img = images[i:i+1]\n            mask = masks[i:i+1]\n\n            # Without TTA\n            pred = torch.sigmoid(model(img))\n            pred_bin = (pred > 0.5).float()\n            intersection = (pred_bin * mask).sum()\n            union = pred_bin.sum() + mask.sum() - intersection\n            all_iou_no_tta.append(((intersection + 1e-6) / (union + 1e-6)).item())\n            all_dice_no_tta.append(((2 * intersection + 1e-6) / (pred_bin.sum() + mask.sum() + 1e-6)).item())\n\n            # With TTA\n            pred_tta = tta(img)\n            pred_bin_tta = (pred_tta > 0.5).float()\n            intersection = (pred_bin_tta * mask).sum()\n            union = pred_bin_tta.sum() + mask.sum() - intersection\n            all_iou_tta.append(((intersection + 1e-6) / (union + 1e-6)).item())\n            all_dice_tta.append(((2 * intersection + 1e-6) / (pred_bin_tta.sum() + mask.sum() + 1e-6)).item())\n\nprint('\\n' + '='*60)\nprint('RESULTS - EXPERIMENT 3 (Attention U-Net)')\nprint('='*60)\nprint('\\nWithout TTA:')\nprint(f'  IoU:  {np.mean(all_iou_no_tta):.4f} +/- {np.std(all_iou_no_tta):.4f}')\nprint(f'  Dice: {np.mean(all_dice_no_tta):.4f} +/- {np.std(all_dice_no_tta):.4f}')\nprint('\\nWith TTA:')\nprint(f'  IoU:  {np.mean(all_iou_tta):.4f} +/- {np.std(all_iou_tta):.4f}')\nprint(f'  Dice: {np.mean(all_dice_tta):.4f} +/- {np.std(all_dice_tta):.4f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCWQf-W7v2qP"
   },
   "source": "## 14. Visualize Predictions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3jwAWUmv2qP",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "0c3d7c09-92f7-4af4-f1de-2eaf0c448cc1"
   },
   "outputs": [],
   "source": "def predict_and_show(dataset, indices):\n    fig, axes = plt.subplots(len(indices), 4, figsize=(20, 5*len(indices)))\n\n    for i, idx in enumerate(indices):\n        img, mask = dataset[idx]\n\n        with torch.no_grad():\n            pred = model(img.unsqueeze(0).to(device))\n            pred = torch.sigmoid(pred).cpu().squeeze().numpy()\n\n        img_np = img.numpy().transpose(1, 2, 0)\n        img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n        img_np = np.clip(img_np, 0, 1)\n\n        mask_np = mask.squeeze().numpy()\n        pred_bin = (pred > 0.5).astype(np.float32)\n\n        overlay = img_np.copy()\n        overlay[pred_bin > 0.5] = overlay[pred_bin > 0.5] * 0.5 + np.array([0, 1, 0]) * 0.5\n\n        axes[i, 0].imshow(img_np)\n        axes[i, 0].set_title('Image')\n        axes[i, 1].imshow(mask_np, cmap='gray')\n        axes[i, 1].set_title('Ground Truth')\n        axes[i, 2].imshow(pred_bin, cmap='gray')\n        axes[i, 2].set_title('Prediction')\n        axes[i, 3].imshow(overlay)\n        axes[i, 3].set_title('Overlay')\n\n        for ax in axes[i]: ax.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\npredict_and_show(val_dataset, [0, 1, 2, 3])"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYZPV59fv2qP"
   },
   "source": "## 15. Save Results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23N1mkTSv2qQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3f434f77-2bc4-4101-dd78-7fb660e3d5df"
   },
   "outputs": [],
   "source": "import pickle\n\nresults_exp3 = {\n    'history': history,\n    'all_iou_no_tta': all_iou_no_tta,\n    'all_dice_no_tta': all_dice_no_tta,\n    'all_iou_tta': all_iou_tta,\n    'all_dice_tta': all_dice_tta,\n    'best_dice': best_dice\n}\n\nwith open('results_exp3.pkl', 'wb') as f:\n    pickle.dump(results_exp3, f)\n\nprint('Results saved to results_exp3.pkl')"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhQD-4wFv2qQ"
   },
   "source": "---\n## Summary of Architecture Modifications\n\n| Component | Description |\n|------------|----------|\n| **Attention Gates** | Attention mechanisms in skip connections that allow the model to focus on relevant regions, suppressing irrelevant responses |\n| **SE Blocks** | Squeeze-and-Excitation blocks for adaptive recalibration of feature channels |\n| **ASPP** | Atrous Spatial Pyramid Pooling at bottleneck for multi-scale context capture |\n| **Focal Loss** | Added to the loss function to handle class imbalance |\n| **Discriminative LR** | Differentiated learning rate for encoder (lower) and decoder (higher) |\n| **OneCycleLR** | More aggressive scheduler for better generalization |\n\nThis is a substantial modification to the network topology, not just an increase in width or depth."
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}